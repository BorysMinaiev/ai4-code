{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom tqdm import tqdm\nfrom pathlib import Path\n\nimport time\n\npd.options.display.width = 180\npd.options.display.max_colwidth = 120\n\ndef is_interactive_mode():\n    return os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Interactive') == 'Interactive'\n\ndata_dir = Path('../input/AI4Code')\n\n\ndf_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n# df_ancestors =df_ancestors.head(1000)\n# TODO: rewrite this to use the dataframe\n\ncnt_by_group = {}\nfor id, row in tqdm(df_ancestors.iterrows()):\n    cnt_by_group[row['ancestor_id']] = cnt_by_group.get(row['ancestor_id'], 0) + 1\n\n\ncnt = pd.Series(cnt_by_group)\nprint('only one:', cnt[cnt == 1].count())\ncnt.plot.hist(grid=True, bins=20, rwidth=0.9,\n                   color='#607c8e')\ncnt\n\ngood_notebooks = []\nfor id, row in tqdm(df_ancestors.iterrows()):\n    if row['parent_id'] != None and cnt_by_group[row['ancestor_id']] == 1:\n        good_notebooks.append(id)\n\ngood_notebooks = pd.Series(good_notebooks)\nprint('good notebooks', len(good_notebooks))\n\nall_train_nb = good_notebooks.sample(frac=0.9, random_state=787788)\nall_validate_nb = good_notebooks.drop(all_train_nb.index)\n\ndef read_notebook(path):\n    return (\n        pd.read_json(\n            path,\n            dtype={'cell_type': 'category', 'source': 'str'})\n        .assign(id=path.stem)\n        .rename_axis('cell_id')\n    )\n\ndef load_train_nbs(num):\n    global df\n\n    paths_train = [data_dir / 'train' / '{}.json'.format(id) for id in all_train_nb.head(num)]\n    notebooks_train = [\n        read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n    ]\n    df = (\n        pd.concat(notebooks_train)\n        .set_index('id', append=True)\n        .swaplevel()\n        .sort_index(level='id', sort_remaining=False)\n    )\n\n    df\n    \nload_train_nbs(100)\n\ndef get_example_nb_id(seed=0):\n    return df.index.get_level_values(0).unique()[seed]\n    \nnb_id = get_example_nb_id()\n\ndef get_nb_by_id(nb_id):\n    return df.loc[nb_id]\n\ndef get_example_cell_from_nb(nb):\n    return nb.index[0]\n\ndef get_example_markdown_cell_from_nb(nb):\n    return nb[nb['cell_type'] == 'markdown'].index[0]\n\ndef get_code_cells(nb):\n    return nb[nb['cell_type'] == 'code'].index\n\ndef get_markdown_cells(nb):\n    return nb[nb['cell_type'] == 'markdown'].index    ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.056058,"end_time":"2022-05-22T21:58:28.980447","exception":false,"start_time":"2022-05-22T21:58:28.924389","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:55:45.174763Z","iopub.execute_input":"2022-06-21T12:55:45.175428Z","iopub.status.idle":"2022-06-21T12:55:45.203581Z","shell.execute_reply.started":"2022-06-21T12:55:45.175328Z","shell.execute_reply":"2022-06-21T12:55:45.202915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copied from: https://github.com/microsoft/CodeBERT/blob/master/UniXcoder/unixcoder.py\n\n# Copyright (c) Microsoft Corporation. \n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n\nclass UniXcoder(nn.Module):\n    def __init__(self, model_name, state_dict=None):\n        \"\"\"\n            Build UniXcoder.\n            Parameters:\n            * `model_name`- huggingface model card name. e.g. microsoft/unixcoder-base\n        \"\"\"        \n        super(UniXcoder, self).__init__()\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n        self.config = RobertaConfig.from_pretrained(model_name)\n        self.config.is_decoder = True\n        self.model = RobertaModel.from_pretrained(model_name, config=self.config)\n\n        if state_dict is not None:\n            self.model.load_state_dict(torch.load(state_dict))   \n        \n        self.register_buffer(\"bias\", torch.tril(torch.ones((1024, 1024), dtype=torch.uint8)).view(1,1024, 1024))\n        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)\n        self.lm_head.weight = self.model.embeddings.word_embeddings.weight\n        self.lsm = nn.LogSoftmax(dim=-1)\n        \n        self.tokenizer.add_tokens([\"<mask0>\"],special_tokens=True)\n          \n    def tokenize(self, inputs, mode=\"<encoder-only>\", max_length=512, padding=False):\n        \"\"\" \n        Convert string to token ids \n                \n        Parameters:\n        * `inputs`- list of input strings.\n        * `max_length`- The maximum total source sequence length after tokenization.\n        * `padding`- whether to pad source sequence length to max_length. \n        * `mode`- which mode the sequence will use. i.e. <encoder-only>, <decoder-only>, <encoder-decoder>\n        \"\"\"\n        assert mode in [\"<encoder-only>\", \"<decoder-only>\", \"<encoder-decoder>\"]\n        \n        tokenizer = self.tokenizer\n        \n        tokens_ids = []\n        for x in inputs:\n            tokens = tokenizer.tokenize(x)\n            if mode == \"<encoder-only>\":\n                tokens = tokens[:max_length-4]\n                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]                \n            elif mode == \"<decoder-only>\":\n                tokens = tokens[-(max_length-3):]\n                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens\n            else:\n                tokens = tokens[:max_length-5]\n                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n                \n            tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n            tokens_ids.append(tokens_id)\n\n        if padding:\n            cur_max_length = len(max(tokens_ids, key=len))\n            tokens_ids = list(map(lambda l: l + [self.config.pad_token_id] * (cur_max_length-len(l)), tokens_ids))\n        return tokens_ids\n            \n    def decode(self, source_ids):   \n        \"\"\" Convert token ids to string \"\"\"      \n        predictions = []\n        for x in source_ids:\n            prediction = []\n            for y in x:\n                t = y.cpu().numpy()\n                t = list(t)\n                if 0 in t:\n                    t = t[:t.index(0)]\n                text = self.tokenizer.decode(t,clean_up_tokenization_spaces=False)\n                prediction.append(text)        \n            predictions.append(prediction)\n        return predictions\n    \n    def forward(self, source_ids):   \n        \"\"\" Obtain token embeddings and sentence embeddings \"\"\"\n        mask = source_ids.ne(self.config.pad_token_id)\n        token_embeddings = self.model(source_ids,attention_mask = mask.unsqueeze(1) * mask.unsqueeze(2))[0]\n        sentence_embeddings = (token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)\n        return token_embeddings, sentence_embeddings       \n\n    def generate(self, source_ids, decoder_only = True, eos_id = None, beam_size = 5, max_length = 64):\n        \"\"\" Generate sequence given context (source_ids) \"\"\"\n        \n        # Set encoder mask attention matrix: bidirectional for <encoder-decoder>, unirectional for <decoder-only>\n        if decoder_only:\n            mask = self.bias[:,:source_ids.size(-1),:source_ids.size(-1)]\n        else:\n            mask = source_ids.ne(self.config.pad_token_id)\n            mask = mask.unsqueeze(1) * mask.unsqueeze(2)  \n            \n        if eos_id is None:\n            eos_id = self.config.eos_token_id\n        \n        device = source_ids.device\n        \n        # Decoding using beam search\n        preds = []       \n        zero = torch.LongTensor(1).fill_(0).to(device)   \n        source_len = list(source_ids.ne(1).sum(-1).cpu().numpy())\n        length = source_ids.size(-1)\n        encoder_output = self.model(source_ids,attention_mask=mask)\n        for i in range(source_ids.shape[0]):\n            context = [[x[i:i+1,:,:source_len[i]].repeat(beam_size,1,1,1) for x in y] \n                     for y in encoder_output.past_key_values]\n            beam = Beam(beam_size,eos_id,device)\n            input_ids = beam.getCurrentState().clone()\n            context_ids = source_ids[i:i+1,:source_len[i]].repeat(beam_size,1)\n            out = encoder_output.last_hidden_state[i:i+1,:source_len[i]].repeat(beam_size,1,1)\n            for _ in range(max_length): \n                if beam.done():\n                    break\n                if _ == 0: \n                    hidden_states = out[:,-1,:]\n                    out = self.lsm(self.lm_head(hidden_states)).data\n                    beam.advance(out)\n                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n                    input_ids = beam.getCurrentState().clone()\n                else:\n                    length = context_ids.size(-1)+input_ids.size(-1)\n                    out = self.model(input_ids,attention_mask=self.bias[:,context_ids.size(-1):length,:length],\n                                       past_key_values=context).last_hidden_state\n                    hidden_states = out[:,-1,:]\n                    out = self.lsm(self.lm_head(hidden_states)).data\n                    beam.advance(out)\n                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n                    input_ids = torch.cat((input_ids,beam.getCurrentState().clone()),-1)\n            hyp = beam.getHyp(beam.getFinal())\n            pred = beam.buildTargetTokens(hyp)[:beam_size]\n            pred = [torch.cat([x.view(-1) for x in p]+[zero]*(max_length-len(p))).view(1,-1) for p in pred]\n            preds.append(torch.cat(pred,0).unsqueeze(0))\n\n        preds = torch.cat(preds,0)    \n\n        return preds  \n    \n\n    \nclass Beam(object):\n    def __init__(self, size, eos, device):\n        self.size = size\n        self.device = device\n        # The score for each translation on the beam.\n        self.scores = torch.FloatTensor(size).zero_().to(device)\n        # The backpointers at each time-step.\n        self.prevKs = []\n        # The outputs at each time-step.\n        self.nextYs = [torch.LongTensor(size).fill_(0).to(device)]\n        # Has EOS topped the beam yet.\n        self._eos = eos\n        self.eosTop = False\n        # Time and k pair for finished.\n        self.finished = []\n\n    def getCurrentState(self):\n        \"Get the outputs for the current timestep.\"\n        batch = self.nextYs[-1].view(-1, 1)\n        return batch\n\n    def getCurrentOrigin(self):\n        \"Get the backpointers for the current timestep.\"\n        return self.prevKs[-1]\n\n    def advance(self, wordLk):\n        \"\"\"\n        Given prob over words for every last beam `wordLk` and attention\n        `attnOut`: Compute and update the beam search.\n        Parameters:\n        * `wordLk`- probs of advancing from the last step (K x words)\n        * `attnOut`- attention at the last step\n        Returns: True if beam search is complete.\n        \"\"\"\n        numWords = wordLk.size(1)\n\n        # Sum the previous scores.\n        if len(self.prevKs) > 0:\n            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n\n            # Don't let EOS have children.\n            for i in range(self.nextYs[-1].size(0)):\n                if self.nextYs[-1][i] == self._eos:\n                    beamLk[i] = -1e20\n        else:\n            beamLk = wordLk[0]\n        flatBeamLk = beamLk.view(-1)\n        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n\n        self.scores = bestScores\n\n        # bestScoresId is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prevK = bestScoresId // numWords\n        self.prevKs.append(prevK)\n        self.nextYs.append((bestScoresId - prevK * numWords))\n\n\n        for i in range(self.nextYs[-1].size(0)):\n            if self.nextYs[-1][i] == self._eos:\n                s = self.scores[i]\n                self.finished.append((s, len(self.nextYs) - 1, i))\n\n        # End condition is when top-of-beam is EOS and no global score.\n        if self.nextYs[-1][0] == self._eos:\n            self.eosTop = True\n\n    def done(self):\n        return self.eosTop and len(self.finished) >= self.size\n\n    def getFinal(self):\n        if len(self.finished) == 0:\n            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n        self.finished.sort(key=lambda a: -a[0])\n        if len(self.finished) != self.size:\n            unfinished=[]\n            for i in range(self.nextYs[-1].size(0)):\n                if self.nextYs[-1][i] != self._eos:\n                    s = self.scores[i]\n                    unfinished.append((s, len(self.nextYs) - 1, i)) \n            unfinished.sort(key=lambda a: -a[0])\n            self.finished+=unfinished[:self.size-len(self.finished)]\n        return self.finished[:self.size]\n\n    def getHyp(self, beam_res):\n        \"\"\"\n        Walk back to construct the full hypothesis.\n        \"\"\"\n        hyps=[]\n        for _,timestep, k in beam_res:\n            hyp = []\n            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n                hyp.append(self.nextYs[j+1][k])\n                k = self.prevKs[j][k]\n            hyps.append(hyp[::-1])\n        return hyps\n    \n    def buildTargetTokens(self, preds):\n        sentence=[]\n        for pred in preds:\n            tokens = []\n            for tok in pred:\n                if tok==self._eos:\n                    break\n                tokens.append(tok)\n            sentence.append(tokens)\n        return sentence\n        ","metadata":{"papermill":{"duration":6.489575,"end_time":"2022-05-22T21:58:39.682208","exception":false,"start_time":"2022-05-22T21:58:33.192633","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:05.00583Z","iopub.execute_input":"2022-06-21T12:56:05.006287Z","iopub.status.idle":"2022-06-21T12:56:11.365997Z","shell.execute_reply.started":"2022-06-21T12:56:05.006251Z","shell.execute_reply":"2022-06-21T12:56:11.365219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef reload_model(preload_from_input=False, preload_from_working=False):\n    global unixcoder_model\n    global device\n    default_model_name = \"../input/unixcoderbase\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    use_pretrained = True\n    state_dict = None\n    if preload_from_input:\n        print(\"Preloading from input...\")\n        state_dict = \"../input/ai4code-pre1000/model-1000.bin\" \n    if preload_from_working:\n        print(\"Preloading from working copy...\")\n        state_dict = \"../working/model-final.bin\"\n    unixcoder_model = UniXcoder(model_name=default_model_name, state_dict=state_dict)\n    unixcoder_model.to(device)\n\nreload_model()","metadata":{"_kg_hide-input":false,"papermill":{"duration":11.138926,"end_time":"2022-05-22T21:58:50.848367","exception":false,"start_time":"2022-05-22T21:58:39.709441","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:11.37052Z","iopub.execute_input":"2022-06-21T12:56:11.372491Z","iopub.status.idle":"2022-06-21T12:56:22.622196Z","shell.execute_reply.started":"2022-06-21T12:56:11.372453Z","shell.execute_reply":"2022-06-21T12:56:22.621373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_orders = pd.read_csv(\n    data_dir / 'train_orders.csv',\n    index_col='id',\n    squeeze=True,\n).str.split()  # Split the string representation of cell_ids into a list","metadata":{"papermill":{"duration":1.954704,"end_time":"2022-05-22T21:58:52.830883","exception":false,"start_time":"2022-05-22T21:58:50.876179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:22.623483Z","iopub.execute_input":"2022-06-21T12:56:22.62433Z","iopub.status.idle":"2022-06-21T12:56:25.334938Z","shell.execute_reply.started":"2022-06-21T12:56:22.624292Z","shell.execute_reply":"2022-06-21T12:56:25.334155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef get_embedding(nb, cell_id):\n    start = time.time()\n    cell = nb.loc[cell_id]\n    tokens = unixcoder_model.tokenize([cell['source']],max_length=512,mode=\"<encoder-only>\")\n    source_ids = torch.tensor(tokens).to(device)\n    _,embeddings = unixcoder_model(source_ids)\n    return torch.nn.functional.normalize(embeddings, p=2, dim=1)[0].cpu()","metadata":{"papermill":{"duration":0.970749,"end_time":"2022-05-22T21:58:54.169385","exception":false,"start_time":"2022-05-22T21:58:53.198636","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:25.336206Z","iopub.execute_input":"2022-06-21T12:56:25.336761Z","iopub.status.idle":"2022-06-21T12:56:25.344253Z","shell.execute_reply.started":"2022-06-21T12:56:25.336718Z","shell.execute_reply":"2022-06-21T12:56:25.343568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_tokens(text):\n    tokens = unixcoder_model.tokenize([text],max_length=512,mode=\"<encoder-only>\")\n    return torch.tensor(tokens).to(device)\n\ndef get_texts_tokens(texts):\n    tokens = unixcoder_model.tokenize(texts,max_length=512,mode=\"<encoder-only>\", padding=True)\n    return torch.tensor(tokens).to(device)\n    \n\ndef get_text_embedding(text):\n    source_ids = get_text_tokens(text)\n    _,embeddings = unixcoder_model(source_ids)\n    return torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:25.347875Z","iopub.execute_input":"2022-06-21T12:56:25.348245Z","iopub.status.idle":"2022-06-21T12:56:25.356401Z","shell.execute_reply.started":"2022-06-21T12:56:25.348208Z","shell.execute_reply":"2022-06-21T12:56:25.35572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef get_nb_embeddings(nb):\n    start = time.time()\n\n    res = {}\n\n#     TODO: maybe different size?\n    batch_size = 8\n    n_chunks = len(nb) / min(len(nb), batch_size)\n\n    nb = nb.sort_values(by=\"source\", key=lambda x: x.str.len())\n    for nb in np.array_split(nb, n_chunks):\n        # TODO: different max_length?\n        tokens = unixcoder_model.tokenize(nb['source'].to_numpy(),max_length=512,mode=\"<encoder-only>\", padding=True)\n        source_ids = torch.tensor(tokens).to(device)\n        _,embeddings = unixcoder_model(source_ids)\n        normalized = torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()\n\n        \n        for key, val in zip(nb['source'].index, normalized):\n            res[key] = val\n    \n    return res\n","metadata":{"papermill":{"duration":5.202493,"end_time":"2022-05-22T21:58:59.401928","exception":false,"start_time":"2022-05-22T21:58:54.199435","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:25.357836Z","iopub.execute_input":"2022-06-21T12:56:25.3581Z","iopub.status.idle":"2022-06-21T12:56:25.369194Z","shell.execute_reply.started":"2022-06-21T12:56:25.358065Z","shell.execute_reply":"2022-06-21T12:56:25.368167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_source(cell_id):\n    return nb.loc[cell_id]['source']\n\ndef sim(emb1, emb2):\n    return torch.einsum(\"i,i->\", emb1, emb2).detach().numpy()","metadata":{"papermill":{"duration":0.060091,"end_time":"2022-05-22T21:58:59.509537","exception":false,"start_time":"2022-05-22T21:58:59.449446","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:25.370556Z","iopub.execute_input":"2022-06-21T12:56:25.370934Z","iopub.status.idle":"2022-06-21T12:56:25.377883Z","shell.execute_reply.started":"2022-06-21T12:56:25.370887Z","shell.execute_reply":"2022-06-21T12:56:25.37719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bisect import bisect\n\n\n# Actually O(N^2), but fast in practice for our data\ndef count_inversions(a):\n    inversions = 0\n    sorted_so_far = []\n    for i, u in enumerate(a):  # O(N)\n        j = bisect(sorted_so_far, u)  # O(log N)\n        inversions += i - j\n        sorted_so_far.insert(j, u)  # O(N)\n    return inversions\n\ndef kendall_tau(ground_truth, predictions):\n    total_inversions = 0  # total inversions in predicted ranks across all instances\n    total_2max = 0  # maximum possible inversions across all instances\n    for gt, pred in zip(ground_truth, predictions):\n        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n        total_inversions += count_inversions(ranks)\n        n = len(gt)\n        total_2max += n * (n - 1)\n    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n\ndef sum_scores(a, b):\n    total_inversions = a[1] + b[1]\n    total_2max = a[2] + b[2]\n    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n\npaths_test = list((data_dir / 'test').glob('*.json'))\nnotebooks_test = [\n    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n]\ntest_df = (\n    pd.concat(notebooks_test)\n    .set_index('id', append=True)\n    .swaplevel()\n    .sort_index(level='id', sort_remaining=False)\n)","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.061111,"end_time":"2022-05-22T21:59:00.107367","exception":false,"start_time":"2022-05-22T21:59:00.046256","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:25.387397Z","iopub.execute_input":"2022-06-21T12:56:25.387641Z","iopub.status.idle":"2022-06-21T12:56:25.396998Z","shell.execute_reply.started":"2022-06-21T12:56:25.387609Z","shell.execute_reply":"2022-06-21T12:56:25.396015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef find_best_cell_order2(nb):\n    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n    markdown_cells = nb[nb['cell_type'] != 'code'].reset_index(level='cell_id')\n    \n    embeddings = get_nb_embeddings(nb)\n    \n    code_cell_ids = code_cells['cell_id'].values\n    \n    order = code_cell_ids.tolist()    \n    \n    for m_cell_id in markdown_cells['cell_id'].values:\n        markdown_emb = embeddings[m_cell_id]\n        best_code = None\n        best_score = -123456.0\n        \n        for c_cell_id in code_cell_ids:\n            code_emb = embeddings[c_cell_id]\n            cur_sim = sim(markdown_emb, code_emb)\n            if cur_sim > best_score:\n                best_score = cur_sim\n                best_code = c_cell_id\n        \n        index = order.index(best_code)\n        order.insert(index, m_cell_id)\n        \n    return order\n\ndef score_answer(nb_id, order):\n    ground_truth = [df_orders[nb_id]]\n    predictions = [order]\n\n    return kendall_tau(ground_truth, predictions)\n\ndef get_random_test_nb_ids(n, random_state=1):\n    return df.reset_index(level='cell_id').sample(n=n, random_state=random_state).index.values.tolist()\n\ndef calc_nb_score(nb_id):\n    return score_answer(nb_id, find_best_cell_order2(df.loc[nb_id]))\n\ndef save_results():\n    res = []\n\n    for id_, notebook in test_df.groupby(level=\"id\"):\n        order = find_best_cell_order2(notebook.reset_index(level='id'))\n        order = \" \".join(order)\n        \n        res.append({'id' : id_, 'cell_order' : order})\n        \n    \n    res = pd.DataFrame(res)\n    res.to_csv('submission.csv', index=False)\n\n    display(res.head())\n    \ndef get_tokens(text):\n    return unixcoder_model.tokenize([text],max_length=512,mode=\"<encoder-only>\")    ","metadata":{"papermill":{"duration":5.598456,"end_time":"2022-05-22T21:59:05.948129","exception":false,"start_time":"2022-05-22T21:59:00.349673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-21T12:56:25.452317Z","iopub.execute_input":"2022-06-21T12:56:25.452692Z","iopub.status.idle":"2022-06-21T12:56:25.460964Z","shell.execute_reply.started":"2022-06-21T12:56:25.452656Z","shell.execute_reply":"2022-06-21T12:56:25.459865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Partially coied from: https://github.com/microsoft/CodeBERT/blob/567dd49a4b916835f93fb95709de714b8772fea2/UniXcoder/downstream-tasks/code-search/model.py\n\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\nimport torch.nn as nn\nimport torch    \nclass Model(nn.Module):   \n    def __init__(self, encoder):\n        super(Model, self).__init__()\n        self.encoder = encoder\n      \n    def forward(self, inputs): \n        outputs = self.encoder(inputs)[1]\n        return torch.nn.functional.normalize(outputs, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:25.511299Z","iopub.execute_input":"2022-06-21T12:56:25.511636Z","iopub.status.idle":"2022-06-21T12:56:25.520577Z","shell.execute_reply.started":"2022-06-21T12:56:25.511584Z","shell.execute_reply":"2022-06-21T12:56:25.5197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, suffix):\n    #output_dir = Path(\"../working/\")                       \n    output_dir = Path(\".\")\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)   \n    model_to_save = model.encoder.model\n    output_dir = os.path.join(output_dir, 'model-{}.bin'.format(suffix)) \n    torch.save(model_to_save.state_dict(), output_dir)\n    print(\"Saved model to {}\".format(output_dir))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:25.52219Z","iopub.execute_input":"2022-06-21T12:56:25.52247Z","iopub.status.idle":"2022-06-21T12:56:25.531802Z","shell.execute_reply.started":"2022-06-21T12:56:25.522429Z","shell.execute_reply":"2022-06-21T12:56:25.530908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport os\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_key)\n\n\ndef init_wandb(name, config={}):\n    is_interactive = \"-interactive\" if is_interactive_mode() else \"\"\n    config=config.copy()\n    config['name']='Train'\n    \n    wandb.init(project=\"ai4code\", name=(name+is_interactive), config=config)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:25.53317Z","iopub.execute_input":"2022-06-21T12:56:25.533409Z","iopub.status.idle":"2022-06-21T12:56:28.891197Z","shell.execute_reply.started":"2022-06-21T12:56:25.533375Z","shell.execute_reply":"2022-06-21T12:56:28.890501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\nfrom transformers import get_linear_schedule_with_warmup\n\nadd_end_token = True\nend_token = 'END'\n\ndef train_on_nb(nb_id, model, optimizer):\n    nb = get_nb_by_id(nb_id)\n    code_cell_ids = get_code_cells(nb).tolist()\n    code_cell_ids.append(end_token)\n    \n    markdown_cell_ids = get_markdown_cells(nb)\n\n    correct_order = df_orders.loc[nb_id]\n    if add_end_token:\n        correct_order.append(end_token)\n    \n    next_code_cell = {}\n    for i in range(len(correct_order)):\n        if correct_order[i] in markdown_cell_ids:\n            next_code_cell[correct_order[i]] = None\n            for j in range(i+1, len(correct_order)):\n                if correct_order[j] in code_cell_ids:\n                    next_code_cell[correct_order[i]] = correct_order[j]\n                    break\n            \n\n    batch_size = 8\n    num_chunks = (len(markdown_cell_ids) + batch_size - 1) // batch_size\n    \n    sum_loss = 0.0\n    \n    for batch_markdown_cell_ids in np.array_split(markdown_cell_ids, num_chunks):\n        batch_code_cells = []\n        for markdown_cell_id in batch_markdown_cell_ids:\n            need_cell = next_code_cell[markdown_cell_id]\n            if need_cell is not None and need_cell not in batch_code_cells:\n                batch_code_cells.append(need_cell)\n        \n        if len(batch_code_cells) == 0:\n            continue;\n            \n        def get_source(cell_id):\n            if cell_id == end_token:\n                return end_token\n            return nb.loc[cell_id]['source']\n\n        code_tokens = get_texts_tokens([get_source(cell_id) for cell_id in batch_code_cells])\n        markdown_tokens = get_texts_tokens([get_source(cell_id) for cell_id in batch_markdown_cell_ids])\n\n        code_vec = model(code_tokens)\n        markdown_vec = model(markdown_tokens)\n        scores = torch.einsum(\"ab,cb->ac\", markdown_vec, code_vec) * 100.0\n\n        expected_order = []\n        for cell_id in batch_markdown_cell_ids:\n            if next_code_cell[cell_id] is None:\n                assert not add_end_token\n                expected_order.append(-100)\n            else:\n                expected_order.append(batch_code_cells.index(next_code_cell[cell_id]))\n\n        expected_order = torch.tensor(expected_order).to(device)\n        \n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(scores, expected_order)\n        \n        sum_loss = sum_loss + loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n#     scheduler.step() \n    return sum_loss / num_chunks\n\n\nfrom torch.optim import AdamW\n\n    \n    \n# for seed in tqdm(range(steps)):    \n#     run_train(seed)\n\n\ndef run_train_all():\n    model = Model(unixcoder_model)\n    model.zero_grad()\n    model.train()\n\n    all = df.index.get_level_values(0).unique()\n\n    learning_rate = 5e-5\n    epochs = 1\n    steps = len(all)\n\n    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n    #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps * epochs)\n\n    start_time = time.time()\n    last_saved_time = start_time\n    save_every_s = 40 * 60\n    max_run_s = 8 * 3600\n    \n    init_wandb(name=\"train-b=8,lr=5e-5,no-sched\", config={'add-end-in-the-end':add_end_token, 'lr':learning_rate})\n    w_loss = 0.0\n    \n    for id, nb_id in enumerate(tqdm(all)):\n        cur_loss = train_on_nb(nb_id, model, optimizer)\n        \n        w_loss = w_loss * 0.95 + cur_loss * 0.05\n        wandb.log({'loss': w_loss})\n        \n        cur_time = time.time()\n        if cur_time - last_saved_time > save_every_s:\n            last_saved_time = cur_time\n            save_model(model, id)\n        \n        if cur_time - start_time > max_run_s:\n            print('Finishing early because of timeout')\n            break\n            \n    wandb.finish()\n    save_model(model, \"final\")\n\n# run_train_all()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:28.895258Z","iopub.execute_input":"2022-06-21T12:56:28.895474Z","iopub.status.idle":"2022-06-21T12:56:28.92441Z","shell.execute_reply.started":"2022-06-21T12:56:28.895448Z","shell.execute_reply":"2022-06-21T12:56:28.923751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def num_test_inputs():\n    return len(test_df.index.get_level_values(0).unique())\n\n# if num_test_inputs() != 4:\nif not is_interactive_mode():\n    print('Going to generate model...')\n    # 2000 - half an hour\n    load_train_nbs(10000)\n    run_train_all()\n    reload_model(preload_from_working=True)\n\n# save_results()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T12:56:28.925865Z","iopub.execute_input":"2022-06-21T12:56:28.926798Z","iopub.status.idle":"2022-06-21T12:56:28.935788Z","shell.execute_reply.started":"2022-06-21T12:56:28.926713Z","shell.execute_reply":"2022-06-21T12:56:28.935076Z"},"trusted":true},"execution_count":null,"outputs":[]}]}