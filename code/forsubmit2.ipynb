{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:54:51.635829Z",
     "iopub.status.busy": "2022-08-03T22:54:51.635435Z",
     "iopub.status.idle": "2022-08-03T22:54:51.641373Z",
     "shell.execute_reply": "2022-08-03T22:54:51.640384Z",
     "shell.execute_reply.started": "2022-08-03T22:54:51.635795Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAPH_MODEL_NAME = \"../input/ai4code-train-ds/graph-model-100k-ncs2.bin\"\n",
    "UNIX_MODEL_NAME = \"../input/ai4code-train-ds/model-epoch1.5.bin\"\n",
    "GRAPH_WEIGHT = 0.4\n",
    "MODEL_MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:10:09.549508Z",
     "iopub.status.busy": "2022-08-03T22:10:09.549149Z",
     "iopub.status.idle": "2022-08-03T22:10:09.567656Z",
     "shell.execute_reply": "2022-08-03T22:10:09.566675Z",
     "shell.execute_reply.started": "2022-08-03T22:10:09.549476Z"
    }
   },
   "outputs": [],
   "source": [
    "# common.py:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np  # linear algebra\n",
    "import torch\n",
    "import math\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "\n",
    "def is_interactive_mode():\n",
    "    return os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Interactive') == 'Interactive'\n",
    "\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "\n",
    "def save_model(model, suffix):\n",
    "    output_dir = Path(\".\")\n",
    "    model_to_save = model.encoder.model\n",
    "    output_dir = os.path.join(output_dir, 'model-{}.bin'.format(suffix))\n",
    "    torch.save(model_to_save.state_dict(), output_dir)\n",
    "    print(\"Saved model to {}\".format(output_dir))\n",
    "\n",
    "\n",
    "def save_roberta_model(model, suffix):\n",
    "    output_dir = Path(\".\")\n",
    "    output_dir = os.path.join(\n",
    "        output_dir, 'roberta-model-{}.bin'.format(suffix))\n",
    "    torch.save(model.state_dict(), output_dir)\n",
    "    print(\"Saved model to {}\".format(output_dir))\n",
    "\n",
    "\n",
    "def get_code_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'code'].index\n",
    "\n",
    "\n",
    "def get_markdown_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'markdown'].index\n",
    "\n",
    "\n",
    "def split_into_batches(lst, batch_size):\n",
    "    num_chunks = (len(lst) + batch_size - 1) // batch_size\n",
    "    return list(np.array_split(lst, num_chunks))\n",
    "\n",
    "\n",
    "def sim(emb1, emb2):\n",
    "    return torch.einsum(\"i,i->\", emb1, emb2).detach().numpy()\n",
    "\n",
    "\n",
    "def get_probs_by_embeddings(embeddings, m_cell_id, code_cell_ids, coef_mul):\n",
    "    markdown_emb = embeddings[m_cell_id]\n",
    "    sims = [sim(markdown_emb, embeddings[c]) for c in code_cell_ids]\n",
    "    max_sim = max(sims)\n",
    "    sims_probs = list(map(lambda x: math.exp((x-max_sim) * coef_mul), sims))\n",
    "    sum_probs = sum(sims_probs)\n",
    "    sims_probs = list(map(lambda x: x/sum_probs, sims_probs))\n",
    "    return sims_probs\n",
    "\n",
    "\n",
    "def get_best_pos_by_probs(probs):\n",
    "    scores = [0.0] * len(probs)\n",
    "    for i in range(len(probs)):\n",
    "        for j in range(len(probs)):\n",
    "            scores[j] += abs(i - j) * probs[i]\n",
    "    return scores.index(min(scores))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OneCell:\n",
    "    score: float\n",
    "    cell_id: str\n",
    "    cell_type: str\n",
    "\n",
    "end_token = 'END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:43:09.284332Z",
     "iopub.status.busy": "2022-08-03T22:43:09.283740Z",
     "iopub.status.idle": "2022-08-03T22:43:09.294249Z",
     "shell.execute_reply": "2022-08-03T22:43:09.293064Z",
     "shell.execute_reply.started": "2022-08-03T22:43:09.284295Z"
    }
   },
   "outputs": [],
   "source": [
    "# config.py:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: Path\n",
    "    unixcoder_model_path: str\n",
    "    wandb_key: str\n",
    "    batch_size: int\n",
    "    batch_size_graph2: int\n",
    "    cosine_minibatch_size: int\n",
    "    cosine_batch_size: int\n",
    "    use_simple_ensemble_model = True\n",
    "\n",
    "\n",
    "def get_local_config():\n",
    "    return Config(data_dir=Path('/home/borys/AI4Code/input/AI4Code'),\n",
    "                  unixcoder_model_path='/home/borys/AI4Code/input/unixcoderbase',\n",
    "                  wandb_key='/home/borys/wandb_key',\n",
    "                  batch_size=2,\n",
    "                  batch_size_graph2=2,\n",
    "                  cosine_minibatch_size=2,\n",
    "                  cosine_batch_size=4\n",
    "                  )\n",
    "\n",
    "\n",
    "def get_jarvis_config():\n",
    "    return Config(data_dir=Path('/home/input/AI4Code'),\n",
    "                  unixcoder_model_path='/home/unixcoderbase',\n",
    "                  wandb_key='/home/wandb_key',\n",
    "                  batch_size=60,\n",
    "                  batch_size_graph2=30,\n",
    "                  cosine_minibatch_size=8,\n",
    "                  cosine_batch_size=60\n",
    "                  )\n",
    "\n",
    "def get_kaggle_config():\n",
    "    return Config(data_dir=Path('../input/AI4Code'),\n",
    "                  unixcoder_model_path='../input/unixcoderbase',\n",
    "                  wandb_key='',\n",
    "                  batch_size=30,\n",
    "                  batch_size_graph2=30,\n",
    "                  cosine_minibatch_size=8,\n",
    "                  cosine_batch_size=30\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "def get_default_config():\n",
    "    if os.getenv('LOGNAME') == 'borys':\n",
    "        print('Get local config')\n",
    "        return get_local_config()\n",
    "    else:\n",
    "        print('Get jarvis config')\n",
    "        return get_jarvis_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:27:04.230246Z",
     "iopub.status.busy": "2022-08-03T22:27:04.229882Z",
     "iopub.status.idle": "2022-08-03T22:27:04.250889Z",
     "shell.execute_reply": "2022-08-03T22:27:04.249699Z",
     "shell.execute_reply.started": "2022-08-03T22:27:04.230213Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    # TODO: wrong types. How does it work? :)\n",
    "    df_orders: list\n",
    "    test_df: list\n",
    "    df_ancestors: list\n",
    "    all_train_nb: list\n",
    "    all_validate_nb: list\n",
    "    cur_train_nbs: list\n",
    "    config: Config\n",
    "    device: str\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load_df_orders()\n",
    "        self.load_df_ancestors()\n",
    "\n",
    "    def load_df_orders(self):\n",
    "        self.df_orders = pd.read_csv(\n",
    "            self.config.data_dir / 'train_orders.csv',\n",
    "            index_col='id',\n",
    "        ).squeeze(\"columns\").str.split()  # Split the string representation of cell_ids into a list\n",
    "\n",
    "    def load_test_nbs(self):\n",
    "        paths_test = list((self.config.data_dir / 'test').glob('*.json'))\n",
    "        notebooks_test = [\n",
    "            read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
    "        ]\n",
    "        self.test_df = (\n",
    "            pd.concat(notebooks_test)\n",
    "            .set_index('id', append=True)\n",
    "            .swaplevel()\n",
    "            .sort_index(level='id', sort_remaining=False)\n",
    "        )\n",
    "\n",
    "    def load_df_ancestors(self):\n",
    "        self.df_ancestors = pd.read_csv(\n",
    "            self.config.data_dir / 'train_ancestors.csv', index_col='id')\n",
    "\n",
    "        # TODO: rewrite this to use the dataframe\n",
    "        cnt_by_group = {}\n",
    "        for id, row in tqdm(self.df_ancestors.iterrows()):\n",
    "            cnt_by_group[row['ancestor_id']] = cnt_by_group.get(\n",
    "                row['ancestor_id'], 0) + 1\n",
    "\n",
    "        cnt = pd.Series(cnt_by_group)\n",
    "        print('only one:', cnt[cnt == 1].count())\n",
    "        cnt.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                      color='#607c8e')\n",
    "        cnt\n",
    "\n",
    "        good_notebooks = []\n",
    "        for id, row in tqdm(self.df_ancestors.iterrows()):\n",
    "            if row['parent_id'] != None and cnt_by_group[row['ancestor_id']] == 1:\n",
    "                good_notebooks.append(id)\n",
    "\n",
    "        good_notebooks = pd.Series(good_notebooks)\n",
    "        print('good notebooks', len(good_notebooks))\n",
    "\n",
    "        self.all_train_nb = good_notebooks.sample(\n",
    "            frac=0.9, random_state=787788)\n",
    "        self.all_validate_nb = good_notebooks.drop(self.all_train_nb.index)\n",
    "\n",
    "    def load_train_nbs_helper(self, ids):\n",
    "        paths_train = [self.config.data_dir / 'train' /\n",
    "                       '{}.json'.format(id) for id in ids]\n",
    "        notebooks_train = [\n",
    "            read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "        ]\n",
    "        self.cur_train_nbs = pd.concat(notebooks_train).set_index(\n",
    "            'id', append=True).swaplevel().sort_index(level='id', sort_remaining=False)\n",
    "\n",
    "    def load_train_nbs(self, num: int):\n",
    "        self.load_train_nbs_helper(self.all_train_nb.head(num))\n",
    "    \n",
    "    def load_train_nbs_range(self, from_: int, to_: int):\n",
    "        self.load_train_nbs_helper(self.all_train_nb[from_:to_])\n",
    "\n",
    "    def load_train_nbs_tail(self, num: int):\n",
    "        self.load_train_nbs_helper(self.all_train_nb.tail(num))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:06:07.905667Z",
     "iopub.status.busy": "2022-08-03T22:06:07.904707Z",
     "iopub.status.idle": "2022-08-03T22:06:07.918636Z",
     "shell.execute_reply": "2022-08-03T22:06:07.917392Z",
     "shell.execute_reply.started": "2022-08-03T22:06:07.905630Z"
    }
   },
   "outputs": [],
   "source": [
    "# metric.py:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from bisect import bisect\n",
    "\n",
    "\n",
    "# Actually O(N^2), but fast in practice for our data\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):  # O(N)\n",
    "        j = bisect(sorted_so_far, u)  # O(log N)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)  # O(N)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Score:\n",
    "    cur_score: float\n",
    "    total_inversions: int\n",
    "    total_pairs: int\n",
    "\n",
    "    def __init__(self, total_inversions, total_pairs):\n",
    "        self.total_inversions = total_inversions\n",
    "        self.total_pairs = total_pairs\n",
    "        if total_pairs == 0:\n",
    "            self.cur_score = 0.0\n",
    "        else:\n",
    "            self.cur_score = 1 - 4 * total_inversions / total_pairs\n",
    "\n",
    "    def merge(a, b):\n",
    "        return Score(a.total_inversions + b.total_inversions, a.total_pairs + b.total_pairs)\n",
    "\n",
    "\n",
    "def kendall_tau_typed(ground_truth, predictions):\n",
    "    total_inversions = 0  # total inversions in predicted ranks across all instances\n",
    "    total_2max = 0  # maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        # rank predicted order in terms of ground truth\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return Score(total_inversions=total_inversions, total_pairs=total_2max)\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    score = kendall_tau_typed(ground_truth, predictions)\n",
    "    return [score.cur_score, score.total_inversions, score.total_pairs]\n",
    "\n",
    "\n",
    "def sum_scores(a, b):\n",
    "    total_inversions = a[1] + b[1]\n",
    "    total_2max = a[2] + b[2]\n",
    "    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n",
    "\n",
    "\n",
    "def calc_nb_score(my_order, correct_order):\n",
    "    ground_truth = [correct_order]\n",
    "    predictions = [my_order]\n",
    "\n",
    "    return kendall_tau_typed(ground_truth, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:50:34.535667Z",
     "iopub.status.busy": "2022-08-03T22:50:34.535277Z",
     "iopub.status.idle": "2022-08-03T22:50:34.592975Z",
     "shell.execute_reply": "2022-08-03T22:50:34.591923Z",
     "shell.execute_reply.started": "2022-08-03T22:50:34.535633Z"
    }
   },
   "outputs": [],
   "source": [
    "# unixcoder.py:\n",
    "\n",
    "# Copied from: https://github.com/microsoft/CodeBERT/blob/master/UniXcoder/unixcoder.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class UniXcoder(nn.Module):\n",
    "    def __init__(self, model_name, state_dict=None):\n",
    "        \"\"\"\n",
    "            Build UniXcoder.\n",
    "            Parameters:\n",
    "            * `model_name`- huggingface model card name. e.g. microsoft/unixcoder-base\n",
    "        \"\"\"\n",
    "        super(UniXcoder, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\n",
    "            model_name, use_fast=True)\n",
    "        self.config = RobertaConfig.from_pretrained(model_name)\n",
    "        self.config.is_decoder = True\n",
    "        self.model = RobertaModel.from_pretrained(\n",
    "            model_name, config=self.config)\n",
    "\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(torch.load(state_dict))\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(\n",
    "            (1024, 1024), dtype=torch.uint8)).view(1, 1024, 1024))\n",
    "        self.lm_head = nn.Linear(\n",
    "            self.config.hidden_size, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embeddings.word_embeddings.weight\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.tokenizer.add_tokens([\"<mask0>\"], special_tokens=True)\n",
    "        #self.tokenizer.add_tokens([\"<END>\"], special_tokens=True)\n",
    "\n",
    "    def tokenize(self, inputs, mode=\"<encoder-only>\", max_length=512, padding=False):\n",
    "        \"\"\" \n",
    "        Convert string to token ids \n",
    "\n",
    "        Parameters:\n",
    "        * `inputs`- list of input strings.\n",
    "        * `max_length`- The maximum total source sequence length after tokenization.\n",
    "        * `padding`- whether to pad source sequence length to max_length. \n",
    "        * `mode`- which mode the sequence will use. i.e. <encoder-only>, <decoder-only>, <encoder-decoder>\n",
    "        \"\"\"\n",
    "        assert mode in [\"<encoder-only>\",\n",
    "                        \"<decoder-only>\", \"<encoder-decoder>\"]\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        tokens_ids = []\n",
    "        for x in inputs:\n",
    "            tokens = tokenizer.tokenize(x)\n",
    "            if mode == \"<encoder-only>\":\n",
    "                tokens = tokens[:max_length-4]\n",
    "                tokens = [tokenizer.cls_token, mode,\n",
    "                          tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "            elif mode == \"<decoder-only>\":\n",
    "                tokens = tokens[-(max_length-3):]\n",
    "                tokens = [tokenizer.cls_token, mode,\n",
    "                          tokenizer.sep_token] + tokens\n",
    "            else:\n",
    "                tokens = tokens[:max_length-5]\n",
    "                tokens = [tokenizer.cls_token, mode,\n",
    "                          tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "\n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            tokens_ids.append(tokens_id)\n",
    "\n",
    "        if padding:\n",
    "            cur_max_length = len(max(tokens_ids, key=len))\n",
    "            tokens_ids = list(map(\n",
    "                lambda l: l + [self.config.pad_token_id] * (cur_max_length-len(l)), tokens_ids))\n",
    "        return tokens_ids\n",
    "\n",
    "    def decode(self, source_ids):\n",
    "        \"\"\" Convert token ids to string \"\"\"\n",
    "        predictions = []\n",
    "        for x in source_ids:\n",
    "            prediction = []\n",
    "            for y in x:\n",
    "                t = y.cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(\n",
    "                    t, clean_up_tokenization_spaces=False)\n",
    "                prediction.append(text)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, source_ids):\n",
    "        \"\"\" Obtain token embeddings and sentence embeddings \"\"\"\n",
    "        mask = source_ids.ne(self.config.pad_token_id)\n",
    "        token_embeddings = self.model(\n",
    "            source_ids, attention_mask=mask.unsqueeze(1) * mask.unsqueeze(2))[0]\n",
    "        sentence_embeddings = (\n",
    "            token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)\n",
    "        return token_embeddings, sentence_embeddings\n",
    "\n",
    "    def generate(self, source_ids, decoder_only=True, eos_id=None, beam_size=5, max_length=64):\n",
    "        \"\"\" Generate sequence given context (source_ids) \"\"\"\n",
    "\n",
    "        # Set encoder mask attention matrix: bidirectional for <encoder-decoder>, unirectional for <decoder-only>\n",
    "        if decoder_only:\n",
    "            mask = self.bias[:, :source_ids.size(-1), :source_ids.size(-1)]\n",
    "        else:\n",
    "            mask = source_ids.ne(self.config.pad_token_id)\n",
    "            mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "\n",
    "        if eos_id is None:\n",
    "            eos_id = self.config.eos_token_id\n",
    "\n",
    "        device = source_ids.device\n",
    "\n",
    "        # Decoding using beam search\n",
    "        preds = []\n",
    "        zero = torch.LongTensor(1).fill_(0).to(device)\n",
    "        source_len = list(source_ids.ne(1).sum(-1).cpu().numpy())\n",
    "        length = source_ids.size(-1)\n",
    "        encoder_output = self.model(source_ids, attention_mask=mask)\n",
    "        for i in range(source_ids.shape[0]):\n",
    "            context = [[x[i:i+1, :, :source_len[i]].repeat(beam_size, 1, 1, 1) for x in y]\n",
    "                       for y in encoder_output.past_key_values]\n",
    "            beam = Beam(beam_size, eos_id, device)\n",
    "            input_ids = beam.getCurrentState().clone()\n",
    "            context_ids = source_ids[i:i+1,\n",
    "                                     :source_len[i]].repeat(beam_size, 1)\n",
    "            out = encoder_output.last_hidden_state[i:i +\n",
    "                                                   1, :source_len[i]].repeat(beam_size, 1, 1)\n",
    "            for _ in range(max_length):\n",
    "                if beam.done():\n",
    "                    break\n",
    "                if _ == 0:\n",
    "                    hidden_states = out[:, -1, :]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(\n",
    "                        0, beam.getCurrentOrigin()))\n",
    "                    input_ids = beam.getCurrentState().clone()\n",
    "                else:\n",
    "                    length = context_ids.size(-1)+input_ids.size(-1)\n",
    "                    out = self.model(input_ids, attention_mask=self.bias[:, context_ids.size(-1):length, :length],\n",
    "                                     past_key_values=context).last_hidden_state\n",
    "                    hidden_states = out[:, -1, :]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(\n",
    "                        0, beam.getCurrentOrigin()))\n",
    "                    input_ids = torch.cat(\n",
    "                        (input_ids, beam.getCurrentState().clone()), -1)\n",
    "            hyp = beam.getHyp(beam.getFinal())\n",
    "            pred = beam.buildTargetTokens(hyp)[:beam_size]\n",
    "            pred = [torch.cat([x.view(-1) for x in p]+[zero]\n",
    "                              * (max_length-len(p))).view(1, -1) for p in pred]\n",
    "            preds.append(torch.cat(pred, 0).unsqueeze(0))\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, size, eos, device):\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.FloatTensor(size).zero_().to(device)\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [torch.LongTensor(size).fill_(0).to(device)]\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eosTop = False\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        batch = self.nextYs[-1].view(-1, 1)\n",
    "        return batch\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "        Parameters:\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = bestScoresId // numWords\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                s = self.scores[i]\n",
    "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >= self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished = []\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i))\n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished += unfinished[:self.size-len(self.finished)]\n",
    "        return self.finished[:self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps = []\n",
    "        for _, timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j+1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "\n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence = []\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok == self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence\n",
    "\n",
    "\n",
    "# Partially coied from: https://github.com/microsoft/CodeBERT/blob/567dd49a4b916835f93fb95709de714b8772fea2/UniXcoder/downstream-tasks/code-search/model.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.encoder(inputs)[1]\n",
    "        return torch.nn.functional.normalize(outputs, p=2, dim=1)\n",
    "\n",
    "\n",
    "def reload_model(state: State, state_dict):\n",
    "    unixcoder_model = UniXcoder(\n",
    "        model_name=state.config.unixcoder_model_path, state_dict=state_dict)\n",
    "    unixcoder_model.to(state.device)\n",
    "    return unixcoder_model\n",
    "\n",
    "\n",
    "def get_text_tokens(state: State, model, text):\n",
    "    tokens = model.tokenize(\n",
    "        [text], max_length=512, mode=\"<encoder-only>\")\n",
    "    return torch.tensor(tokens).to(state.device)\n",
    "\n",
    "\n",
    "def get_text_embedding(state: State, model, text):\n",
    "    source_ids = get_text_tokens(state, model, text)\n",
    "    _, embeddings = model(source_ids)\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()[0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_unix_nb_embeddings(state: State, model, nb):\n",
    "    res = {}\n",
    "\n",
    "    batch_size = state.config.batch_size\n",
    "    n_chunks = len(nb) / min(len(nb), batch_size)\n",
    "\n",
    "    nb = nb.sort_values(by=\"source\", key=lambda x: x.str.len())\n",
    "    for nb in np.array_split(nb, n_chunks):\n",
    "        texts = nb['source'].to_numpy()\n",
    "\n",
    "        tokens = model.tokenize(texts, max_length=MODEL_MAX_LEN,\n",
    "                                mode=\"<encoder-only>\", padding=True)\n",
    "        source_ids = torch.tensor(tokens).to(state.device)\n",
    "        _, embeddings = model(source_ids)\n",
    "        normalized = torch.nn.functional.normalize(\n",
    "            embeddings, p=2, dim=1).cpu()\n",
    "\n",
    "        for key, val in zip(nb['source'].index, normalized):\n",
    "            res[key] = val\n",
    "\n",
    "    res['END'] = get_text_embedding(state, model, 'END')\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, state, state_dict=None):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.encoder = reload_model(state, state_dict=None)\n",
    "        self.top = nn.Linear(768 + 6, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.name = \"\"\n",
    "        if state_dict is not None:\n",
    "            self.name = state_dict\n",
    "            self.load_state_dict(torch.load(\n",
    "                state_dict, map_location=state.device))\n",
    "        self.to(state.device)\n",
    "\n",
    "    def forward(self, inputs, additional_features, device):\n",
    "        outputs = self.encoder(inputs)[1]\n",
    "        joined = torch.cat((outputs, additional_features), 1).to(device)\n",
    "        per_model = self.top(joined)\n",
    "        return self.softmax(per_model)\n",
    "\n",
    "    def save(self, suffix):\n",
    "        output_dir = Path(\".\")\n",
    "        output_path = os.path.join(\n",
    "            output_dir, 'ensemble-model-{}.bin'.format(suffix))\n",
    "        torch.save(self.state_dict(), output_path)\n",
    "        print(\"Saved model to {}\".format(output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:07:58.298138Z",
     "iopub.status.busy": "2022-08-03T22:07:58.297228Z",
     "iopub.status.idle": "2022-08-03T22:07:58.347323Z",
     "shell.execute_reply": "2022-08-03T22:07:58.346148Z",
     "shell.execute_reply.started": "2022-08-03T22:07:58.298092Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph_model.py:\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    markdown: str\n",
    "    code: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleWithLabel:\n",
    "    sample: Sample\n",
    "    label: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TwoSamples:\n",
    "    markdown: str\n",
    "    correct_code: str\n",
    "    wrong_code: str\n",
    "\n",
    "    def max_len(self):\n",
    "        l1 = len(self.markdown) + len(self.correct_code)\n",
    "        l2 = len(self.markdown) + len(self.wrong_code)\n",
    "        return max(l1, l2)\n",
    "\n",
    "\n",
    "max_tokenizer_len = 256\n",
    "\n",
    "\n",
    "class MyGraphModel(nn.Module):\n",
    "    def __init__(self, state: State, preload_state=None, next_code_cells=1, coef_mul=25):\n",
    "        super(MyGraphModel, self).__init__()\n",
    "        self.graph = AutoModel.from_pretrained(\n",
    "            '../input/graphcodebert-base/graphcodebert-base')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            '../input/graphcodebert-base/graphcodebert-base')\n",
    "        self.top = nn.Linear(768, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.next_code_cells = next_code_cells\n",
    "        self.coef_mul = coef_mul\n",
    "        if preload_state is not None:\n",
    "            print('Preloading state:', preload_state)\n",
    "            #state = torch.load(preload_state, map_location=state.device)\n",
    "            cur_state = torch.load(preload_state)\n",
    "            if 'state_dict' in cur_state:\n",
    "                self.load_state_dict(cur_state['state_dict'])\n",
    "                self.next_code_cells = cur_state['next_code_cells']\n",
    "                self.coef_mul = cur_state['coef_mul']\n",
    "            else:\n",
    "                self.load_state_dict(cur_state)\n",
    "        self.name = preload_state if preload_state is not None else \"0\"\n",
    "        self.name += \";ncs=\" + str(self.next_code_cells)\n",
    "        self.to(state.device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, use_sigmoid, return_scalar):\n",
    "        x = self.graph(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        x = x[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        if return_scalar:\n",
    "            x = self.top(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.normalize(x, p=2, dim=1)\n",
    "        if use_sigmoid:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def encode_sample(self, sample):\n",
    "        max_length = (max_tokenizer_len // 2 - 3)\n",
    "        code_tokens = self.tokenizer.tokenize(\n",
    "            sample.code, max_length=max_length, truncation=True)\n",
    "        markdown_tokens = self.tokenizer.tokenize(\n",
    "            sample.markdown, max_length=max_length, truncation=True)\n",
    "        tokens = [self.tokenizer.cls_token] + markdown_tokens + \\\n",
    "            [self.tokenizer.sep_token] + code_tokens + [self.tokenizer.sep_token]\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        return {'input_ids': token_ids, 'attention_mask': [1] * len(token_ids)}\n",
    "\n",
    "    def encode(self, state: State, samples):\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "\n",
    "        for sample in samples:\n",
    "            encoded = self.encode_sample(sample)\n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_mask.append(encoded['attention_mask'])\n",
    "\n",
    "        max_len = max(map(lambda x: len(x), input_ids))\n",
    "        for i in range(len(input_ids)):\n",
    "            more = max_len - len(input_ids[i])\n",
    "            input_ids[i] += [self.tokenizer.pad_token_id] * more\n",
    "            attention_mask[i] += [0] * more\n",
    "\n",
    "        return {'input_ids': torch.LongTensor(input_ids).to(state.device),\n",
    "                'attention_mask': torch.LongTensor(attention_mask).to(state.device)\n",
    "                }\n",
    "\n",
    "    def encode_texts(self, state: State, texts):\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "\n",
    "        for text in texts:\n",
    "            tokens = [self.tokenizer.cls_token_id] + self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(\n",
    "                text, max_length=MODEL_MAX_LEN-2, truncation=True)) + [self.tokenizer.sep_token_id]\n",
    "            input_ids.append(tokens)\n",
    "            attention_mask.append([1] * len(tokens))\n",
    "\n",
    "        max_len = max(map(lambda x: len(x), input_ids))\n",
    "        for i in range(len(input_ids)):\n",
    "            more = max_len - len(input_ids[i])\n",
    "            input_ids[i] += [self.tokenizer.pad_token_id] * more\n",
    "            attention_mask[i] += [0] * more\n",
    "\n",
    "        return {'input_ids': torch.LongTensor(input_ids).to(state.device),\n",
    "                'attention_mask': torch.LongTensor(attention_mask).to(state.device)\n",
    "                }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, state: State, samples, use_sigmoid):\n",
    "        result = []\n",
    "        batches = split_into_batches(samples, state.config.batch_size)\n",
    "        for batch in batches:\n",
    "            encoded = self.encode(state, batch)\n",
    "            pred = self(encoded['input_ids'],\n",
    "                        encoded['attention_mask'], use_sigmoid)\n",
    "            result += [x[0].item() for x in pred]\n",
    "        return result\n",
    "\n",
    "    def save(self, suffix, optimizer=None):\n",
    "        output_dir = Path(\".\")\n",
    "        output_path = os.path.join(\n",
    "            output_dir, 'graph-model-{}.bin'.format(suffix))\n",
    "        torch.save({'state_dict':self.state_dict(), 'next_code_cells':self.next_code_cells, 'coef_mul':self.coef_mul}, output_path)\n",
    "        print(\"Saved model to {}\".format(output_path))\n",
    "        if optimizer is not None:\n",
    "            output_path = os.path.join(\n",
    "                output_dir, 'graph-model-{}.opt.bin'.format(suffix))\n",
    "            torch.save(optimizer.state_dict(), output_path)\n",
    "\n",
    "\n",
    "def train(state, model, dataset, save_to_wandb=False, optimizer_state=None):\n",
    "    print('start training...')\n",
    "    np.random.seed(123)\n",
    "    learning_rate = 5e-5\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    if optimizer_state is not None:\n",
    "        print('loading optimizer state...')\n",
    "        optimizer.load_state_dict(torch.load(optimizer_state))\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(dataset))\n",
    "    # scheduler = get_linear_schedule_with_warmup(\n",
    "    #    optimizer, num_warmup_steps=0.05*len(dataset), num_training_steps=len(dataset))\n",
    "    model.train()\n",
    "    print('training... num batches:', len(dataset))\n",
    "    if save_to_wandb:\n",
    "        init_wandb(name='graph-training')\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    for b_id, batch in enumerate(tqdm(dataset)):\n",
    "        samples = list(map(lambda x: x.sample, batch))\n",
    "        encoded = model.encode(state, samples)\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        target = list(map(lambda x: [x.label], batch))\n",
    "        target = torch.FloatTensor(target).to(state.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if save_to_wandb:\n",
    "            wandb.log({'graph_loss': loss.item()})\n",
    "\n",
    "        if (b_id % 1000 == 999):\n",
    "            print('Saving model after', b_id)\n",
    "            model.save('step-batch-' + str(b_id), optimizer=optimizer)\n",
    "\n",
    "    if save_to_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    model.save('cur-final', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def train2(state, model, dataset, save_to_wandb=False, optimizer_state=None):\n",
    "    print('start training...')\n",
    "    np.random.seed(123)\n",
    "    learning_rate = 3e-5\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=learning_rate, betas=(0.9, 0.995))\n",
    "    if optimizer_state is not None:\n",
    "        print('loading optimizer state...')\n",
    "        optimizer.load_state_dict(torch.load(optimizer_state))\n",
    "    # scheduler = get_linear_schedule_with_warmup(\n",
    "    #     optimizer, num_warmup_steps=0.05*len(dataset), num_training_steps=len(dataset))\n",
    "    model.train()\n",
    "    print('training... num batches:', len(dataset))\n",
    "    if save_to_wandb:\n",
    "        init_wandb(name='graph2-training')\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    accumulation_steps = 1\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer, T_max=len(dataset)/accumulation_steps)\n",
    "\n",
    "    for b_id, batch in enumerate(tqdm(dataset)):\n",
    "        samples = list(map(lambda x: [Sample(markdown=x.markdown, code=x.correct_code), Sample(\n",
    "            markdown=x.markdown, code=x.wrong_code)], batch))\n",
    "        samples = list(itertools.chain(*samples))\n",
    "        encoded = model.encode(state, samples)\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(input_ids, attention_mask, use_sigmoid=False)\n",
    "\n",
    "            losses = []\n",
    "            for i in range(len(batch)):\n",
    "                sm = F.softmax(pred[i*2:i*2+2] * 10.0, dim=0)\n",
    "                losses.append(sm[1])\n",
    "\n",
    "            total_loss = sum(losses) / len(batch)\n",
    "        scaler.scale(total_loss).backward()\n",
    "        if b_id % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        #\n",
    "\n",
    "        if save_to_wandb:\n",
    "            wandb.log({'graph2_loss': total_loss.item()})\n",
    "\n",
    "        if (b_id % 5000 == 4999):\n",
    "            print('Saving model after', b_id)\n",
    "            model.save('2-step-batch-' + str(b_id), optimizer=optimizer)\n",
    "\n",
    "    if save_to_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    model.save('2-cur-final', optimizer=optimizer)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Embedding:\n",
    "    cell_id: str\n",
    "    text: str\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_graph_nb_embeddings(state: State, model, nb):\n",
    "    def get_code(cell_id):\n",
    "        if cell_id == end_token:\n",
    "            return end_token\n",
    "        return nb.loc[cell_id]['source']\n",
    "\n",
    "    code_cells = get_code_cells(nb).tolist()\n",
    "    code_cells.append(end_token)\n",
    "\n",
    "    to_convert = [Embedding(cell_id=end_token, text=end_token)]\n",
    "\n",
    "    for cell_id in nb.index:\n",
    "        text=get_code(cell_id)\n",
    "        if cell_id in code_cells:\n",
    "            idx = code_cells.index(cell_id)\n",
    "            more_code_cells = code_cells[idx+1:idx+model.next_code_cells]\n",
    "            for next_id in more_code_cells:\n",
    "                text += model.tokenizer.sep_token + get_code(next_id)\n",
    "        to_convert.append(\n",
    "            Embedding(cell_id=cell_id, text=text))\n",
    "    to_convert.sort(key=lambda x: len(x.text))\n",
    "\n",
    "    num_chunks = (len(to_convert) + state.config.batch_size -\n",
    "                  1) // state.config.batch_size\n",
    "\n",
    "    result = {}\n",
    "    for batch in np.array_split(to_convert, num_chunks):\n",
    "        all_texts = list(map(lambda x: x.text, batch))\n",
    "        encoded = model.encode_texts(state, all_texts)\n",
    "        embeddings = model(\n",
    "            input_ids=encoded['input_ids'], attention_mask=encoded['attention_mask'], use_sigmoid=False, return_scalar=False)\n",
    "        for i in range(len(batch)):\n",
    "            result[batch[i].cell_id] = embeddings[i].cpu()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:14:11.774808Z",
     "iopub.status.busy": "2022-08-03T22:14:11.774381Z",
     "iopub.status.idle": "2022-08-03T22:14:11.798471Z",
     "shell.execute_reply": "2022-08-03T22:14:11.797256Z",
     "shell.execute_reply.started": "2022-08-03T22:14:11.774772Z"
    }
   },
   "outputs": [],
   "source": [
    "# ensembles.py:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    text: str\n",
    "    md_cell_id: str\n",
    "    graph3_pos: float\n",
    "    unix_pos: float\n",
    "    total_cells: int\n",
    "    md_cells: int\n",
    "    code_cells: int\n",
    "    part_code_cells: float\n",
    "    target_pos: float\n",
    "\n",
    "\n",
    "def gen_nb_samples(nb, graph3_embeddings, unix_embeddings, correct_order):\n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    markdown_cells = nb[nb['cell_type'] != 'code'].reset_index(level='cell_id')\n",
    "\n",
    "    code_cell_ids = code_cells['cell_id'].values.tolist()\n",
    "    code_cell_ids.append('END')\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    md_cells = len(markdown_cells)\n",
    "    code_cells = len(code_cells)\n",
    "    total_cells = md_cells + code_cells\n",
    "    part_code_cells = code_cells / total_cells\n",
    "\n",
    "    for m_cell_id in markdown_cells['cell_id'].values:\n",
    "        text = nb.loc[m_cell_id]['source']\n",
    "        graph_sims_probs = get_probs_by_embeddings(\n",
    "            graph3_embeddings, m_cell_id, code_cell_ids, 25.0)\n",
    "        unix_sims_probs = get_probs_by_embeddings(\n",
    "            unix_embeddings, m_cell_id, code_cell_ids, 1000.0)\n",
    "\n",
    "        graph3_pos = get_best_pos_by_probs(graph_sims_probs)\n",
    "        unix_pos = get_best_pos_by_probs(unix_sims_probs)\n",
    "\n",
    "        best_coef = 0\n",
    "\n",
    "        if correct_order is not None:\n",
    "            idx = correct_order.index(m_cell_id)\n",
    "            next_code_cell = 'END'\n",
    "            for i in range(idx+1, len(correct_order)):\n",
    "                if correct_order[i] in code_cell_ids:\n",
    "                    next_code_cell = correct_order[i]\n",
    "                    break\n",
    "            target_score = code_cell_ids.index(next_code_cell)\n",
    "            OPTIONS = 20\n",
    "            best_diff = 123.45\n",
    "            sum_best_coefs = 0.0\n",
    "            cnt_best_coefs = 0.0\n",
    "            all_possible_positions = []\n",
    "            for o in range(OPTIONS+1):\n",
    "                coef = o/(OPTIONS)\n",
    "                sim_probs = [graph_sims_probs[i] * coef + unix_sims_probs[i] * (1 - coef) for i in range(len(graph_sims_probs))]\n",
    "                                \n",
    "                pos = get_best_pos_by_probs(sim_probs)\n",
    "                all_possible_positions.append(pos)\n",
    "                diff = abs(pos - target_score)\n",
    "                if diff < best_diff:\n",
    "                    best_diff = diff\n",
    "                    sum_best_coefs = 0.0\n",
    "                    cnt_best_coefs = 0.0\n",
    "                if diff == best_diff:\n",
    "                    sum_best_coefs += coef\n",
    "                    cnt_best_coefs += 1.0\n",
    "            # all_possible_positions.sort()\n",
    "            if all_possible_positions[0] == all_possible_positions[-1]:\n",
    "                continue\n",
    "            # print('target score:', target_score)\n",
    "            # print(all_possible_positions)\n",
    "            best_coef = sum_best_coefs / cnt_best_coefs\n",
    "            # print('best coef:', best_coef)\n",
    "        samples.append(Sample(md_cell_id=m_cell_id, text=text, graph3_pos=graph3_pos, unix_pos=unix_pos, total_cells=total_cells,\n",
    "                       md_cells=md_cells, code_cells=code_cells, part_code_cells=part_code_cells, target_pos=best_coef))\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_samples(state: State, nb, graph3_model: MyGraphModel, unixcoder_model, correct_order):\n",
    "    graph3_embeddings = get_graph_nb_embeddings(state, graph3_model, nb)\n",
    "    unix_embeddings = get_unix_nb_embeddings(state, unixcoder_model, nb)\n",
    "    return gen_nb_samples(nb, graph3_embeddings, unix_embeddings, correct_order)\n",
    "\n",
    "\n",
    "def predict(state: State, ensemble_model, samples):\n",
    "    texts = list(map(lambda s: s.text, samples))\n",
    "    additional_features = list(map(lambda s: torch.FloatTensor(\n",
    "        [s.graph3_pos, s.unix_pos, s.total_cells, s.md_cells, s.code_cells, s.part_code_cells]), samples))\n",
    "    additional_features = torch.stack(additional_features).to(state.device)\n",
    "\n",
    "    to_mul = list(map(lambda s: torch.FloatTensor(\n",
    "        [s.graph3_pos, s.unix_pos]), samples))\n",
    "    to_mul = torch.stack(to_mul).to(state.device)\n",
    "\n",
    "    coefs = None\n",
    "    if state.config.use_simple_ensemble_model:\n",
    "        coefs = ensemble_model(additional_features)\n",
    "        #coefs = [torch.FloatTensor([0.4, 0.6]) for _ in range(len(samples))]\n",
    "        #coefs = torch.stack(coefs).to(state.device)\n",
    "    else:\n",
    "        text_tokens = ensemble_model.encoder.tokenize(\n",
    "            texts, max_length=512, mode=\"<encoder-only>\", padding=True)\n",
    "        text_tokens = torch.tensor(text_tokens).to(state.device)\n",
    "        coefs = ensemble_model(text_tokens, additional_features, state.device)\n",
    "    preds = torch.einsum(\"ab,ab->a\", coefs, to_mul)\n",
    "\n",
    "    return {'coefs': coefs, 'preds': preds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T22:59:51.241510Z",
     "iopub.status.busy": "2022-08-03T22:59:51.241136Z",
     "iopub.status.idle": "2022-08-03T23:00:12.436819Z",
     "shell.execute_reply": "2022-08-03T23:00:12.435871Z",
     "shell.execute_reply.started": "2022-08-03T22:59:51.241478Z"
    }
   },
   "outputs": [],
   "source": [
    "config = get_kaggle_config()\n",
    "state = State(config)\n",
    "\n",
    "\n",
    "graph3_model = MyGraphModel(state, preload_state=GRAPH_MODEL_NAME)\n",
    "graph3_model.to(state.device)\n",
    "print('Graph3 model loaded')\n",
    "\n",
    "unixcoder_model = reload_model(state, UNIX_MODEL_NAME)\n",
    "print('Unixcoder model loaded')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def get_probs_by_embeddings(embeddings, m_cell_id, code_cell_ids, coef_mul):\n",
    "    markdown_emb = embeddings[m_cell_id]\n",
    "    sims = [sim(markdown_emb, embeddings[c]) for c in code_cell_ids]\n",
    "    max_sim = max(sims)\n",
    "    sims_probs = list(map(lambda x:math.exp((x-max_sim) * coef_mul), sims))\n",
    "    sum_probs = sum(sims_probs)\n",
    "    sims_probs = list(map(lambda x:x/sum_probs, sims_probs))\n",
    "    return sims_probs    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_order(state: State, nb, graph3_model: MyGraphModel, unixcoder_model, graph3_embeddings, unix_embeddings, graph_weight):\n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    \n",
    "    code_cell_ids = code_cells['cell_id'].values.tolist()\n",
    "    code_cell_ids.append('END')\n",
    "    \n",
    "    cells = []\n",
    "    for pos, cell_id in enumerate(get_code_cells(nb)):\n",
    "        cells.append(OneCell(score=pos+0.5, cell_id=cell_id, cell_type=\"code\"))\n",
    "\n",
    "    markdown_cells = get_markdown_cells(nb)\n",
    "\n",
    "    for cell_id in markdown_cells:            \n",
    "        coef = graph_weight\n",
    "\n",
    "        graph_sims_probs = get_probs_by_embeddings(graph3_embeddings, cell_id, code_cell_ids, graph3_model.coef_mul)\n",
    "        unix_sims_probs = get_probs_by_embeddings(unix_embeddings, cell_id, code_cell_ids, 1000.0)\n",
    "        sims_probs = [a*coef + b*(1 - coef) for (a, b) in zip(graph_sims_probs, unix_sims_probs)]\n",
    "        scores = [0.0] * len(sims_probs)\n",
    "        for i in range(len(sims_probs)):\n",
    "            for j in range(len(sims_probs)):\n",
    "                scores[j] += abs(i - j) * sims_probs[i]\n",
    "        best_pos = scores.index(min(scores))\n",
    "\n",
    "\n",
    "        cells.append(OneCell(score=best_pos, cell_id=cell_id, cell_type=\"markdown\"))\n",
    "\n",
    "    cells.sort(key=lambda x:x.score)\n",
    "    return list(map(lambda c:c.cell_id, cells))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T23:01:47.636144Z",
     "iopub.status.busy": "2022-08-03T23:01:47.635471Z",
     "iopub.status.idle": "2022-08-03T23:01:48.283888Z",
     "shell.execute_reply": "2022-08-03T23:01:48.283015Z",
     "shell.execute_reply.started": "2022-08-03T23:01:47.636106Z"
    }
   },
   "outputs": [],
   "source": [
    "state.load_test_nbs()\n",
    "# state.load_train_nbs_tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-03T23:01:56.613626Z",
     "iopub.status.busy": "2022-08-03T23:01:56.613257Z",
     "iopub.status.idle": "2022-08-03T23:03:14.698345Z",
     "shell.execute_reply": "2022-08-03T23:03:14.697251Z",
     "shell.execute_reply.started": "2022-08-03T23:01:56.613591Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def save_results(state, graph3_model, unixcoder_model):\n",
    "    res = []\n",
    "    graph3_model.eval()\n",
    "    unixcoder_model.eval()\n",
    "     \n",
    "    print('Start using the model:')\n",
    "\n",
    "    df = state.test_df\n",
    "    # df = state.cur_train_nbs # TODO: change!\n",
    "    all = df.index.get_level_values(0).unique()\n",
    "\n",
    "    for cnt, nb_id in enumerate(tqdm(all)):\n",
    "        nb = df.loc[nb_id]\n",
    "        graph3_embeddings = get_graph_nb_embeddings(state, graph3_model, nb)\n",
    "        unix_embeddings = get_unix_nb_embeddings(state, unixcoder_model, nb)\n",
    "\n",
    "        my_order = predict_order(state, nb, graph3_model, unixcoder_model, graph3_embeddings, unix_embeddings, graph_weight=GRAPH_WEIGHT)\n",
    "        \n",
    "        my_order = \" \".join(my_order)\n",
    "        res.append({'id' : nb_id, 'cell_order' : my_order})\n",
    "        \n",
    "    \n",
    "    res = pd.DataFrame(res)\n",
    "    res.to_csv('submission.csv', index=False)\n",
    "\n",
    "    display(res.head())\n",
    "    \n",
    "save_results(state, graph3_model, unixcoder_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
