{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:49:29.036681Z",
     "iopub.status.busy": "2022-07-13T18:49:29.036295Z",
     "iopub.status.idle": "2022-07-13T18:49:29.078845Z",
     "shell.execute_reply": "2022-07-13T18:49:29.078106Z",
     "shell.execute_reply.started": "2022-07-13T18:49:29.036582Z"
    }
   },
   "outputs": [],
   "source": [
    "default_cnt = 100000\n",
    "default_mul = 1000\n",
    "default_model_name = \"model-\" + str(default_cnt // 1000) + \"k-mul\" + str(default_mul) + \"-all-bs60-8.bin\"\n",
    "default_wandb_name = \"testing: \" + default_model_name\n",
    "print('name:', default_wandb_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-13T18:49:29.081737Z",
     "iopub.status.busy": "2022-07-13T18:49:29.081498Z",
     "iopub.status.idle": "2022-07-13T18:49:49.100011Z",
     "shell.execute_reply": "2022-07-13T18:49:49.099260Z",
     "shell.execute_reply.started": "2022-07-13T18:49:29.081710Z"
    },
    "papermill": {
     "duration": 0.056058,
     "end_time": "2022-05-22T21:58:28.980447",
     "exception": false,
     "start_time": "2022-05-22T21:58:28.924389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "def is_interactive_mode():\n",
    "    return os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Interactive') == 'Interactive'\n",
    "\n",
    "data_dir = Path('../input/AI4Code')\n",
    "\n",
    "df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "\n",
    "cnt_by_group = {}\n",
    "for id, row in tqdm(df_ancestors.iterrows()):\n",
    "    cnt_by_group[row['ancestor_id']] = cnt_by_group.get(row['ancestor_id'], 0) + 1\n",
    "\n",
    "\n",
    "cnt = pd.Series(cnt_by_group)\n",
    "print('only one:', cnt[cnt == 1].count())\n",
    "cnt.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                   color='#607c8e')\n",
    "cnt\n",
    "\n",
    "good_notebooks = []\n",
    "for id, row in tqdm(df_ancestors.iterrows()):\n",
    "    if row['parent_id'] != None and cnt_by_group[row['ancestor_id']] == 1:\n",
    "        good_notebooks.append(id)\n",
    "\n",
    "good_notebooks = pd.Series(good_notebooks)\n",
    "\n",
    "print('good notebooks', len(good_notebooks))\n",
    "\n",
    "all_train_nb = good_notebooks.sample(frac=0.9, random_state=787788)\n",
    "all_validate_nb = good_notebooks.drop(all_train_nb.index)\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "def load_train_nbs(num):\n",
    "    global df\n",
    "\n",
    "    paths_train = [data_dir / 'train' / '{}.json'.format(id) for id in all_train_nb.head(num)]\n",
    "    notebooks_train = [\n",
    "        read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "        pd.concat(notebooks_train)\n",
    "        .set_index('id', append=True)\n",
    "        .swaplevel()\n",
    "        .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "\n",
    "    df\n",
    "    \n",
    "def load_train_nbs_tail(num):\n",
    "    global df\n",
    "\n",
    "    paths_train = [data_dir / 'train' / '{}.json'.format(id) for id in all_train_nb.tail(num)]\n",
    "    notebooks_train = [\n",
    "        read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "        pd.concat(notebooks_train)\n",
    "        .set_index('id', append=True)\n",
    "        .swaplevel()\n",
    "        .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "\n",
    "    df\n",
    "    \n",
    "def load_validate_nbs(num=100000):\n",
    "    global df\n",
    "\n",
    "    paths_train = [data_dir / 'train' / '{}.json'.format(id) for id in all_validate_nb.head(num)]\n",
    "    notebooks_train = [\n",
    "        read_notebook(path) for path in tqdm(paths_train, desc='Validate NBs')\n",
    "    ]\n",
    "    df = (\n",
    "        pd.concat(notebooks_train)\n",
    "        .set_index('id', append=True)\n",
    "        .swaplevel()\n",
    "        .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "\n",
    "    df\n",
    "    \n",
    "load_train_nbs(100)\n",
    "\n",
    "def get_example_nb_id(seed=0):\n",
    "    return df.index.get_level_values(0).unique()[seed]\n",
    "    \n",
    "nb_id = get_example_nb_id()\n",
    "\n",
    "def get_nb_by_id(nb_id):\n",
    "    return df.loc[nb_id]\n",
    "\n",
    "def get_example_cell_from_nb(nb):\n",
    "    return nb.index[0]\n",
    "\n",
    "def get_example_markdown_cell_from_nb(nb):\n",
    "    return nb[nb['cell_type'] == 'markdown'].index[0]\n",
    "\n",
    "def get_code_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'code'].index\n",
    "\n",
    "def get_markdown_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'markdown'].index    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:49:49.102246Z",
     "iopub.status.busy": "2022-07-13T18:49:49.101851Z",
     "iopub.status.idle": "2022-07-13T18:49:57.290972Z",
     "shell.execute_reply": "2022-07-13T18:49:57.290181Z",
     "shell.execute_reply.started": "2022-07-13T18:49:49.102207Z"
    },
    "papermill": {
     "duration": 6.489575,
     "end_time": "2022-05-22T21:58:39.682208",
     "exception": false,
     "start_time": "2022-05-22T21:58:33.192633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copied from: https://github.com/microsoft/CodeBERT/blob/master/UniXcoder/unixcoder.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. \n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "class UniXcoder(nn.Module):\n",
    "    def __init__(self, model_name, state_dict=None):\n",
    "        \"\"\"\n",
    "            Build UniXcoder.\n",
    "            Parameters:\n",
    "            * `model_name`- huggingface model card name. e.g. microsoft/unixcoder-base\n",
    "        \"\"\"        \n",
    "        super(UniXcoder, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.config = RobertaConfig.from_pretrained(model_name)\n",
    "        self.config.is_decoder = True\n",
    "        self.model = RobertaModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(torch.load(state_dict))   \n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones((1024, 1024), dtype=torch.uint8)).view(1,1024, 1024))\n",
    "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embeddings.word_embeddings.weight\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.tokenizer.add_tokens([\"<mask0>\"],special_tokens=True)\n",
    "          \n",
    "    def tokenize(self, inputs, mode=\"<encoder-only>\", max_length=512, padding=False):\n",
    "        \"\"\" \n",
    "        Convert string to token ids \n",
    "                \n",
    "        Parameters:\n",
    "        * `inputs`- list of input strings.\n",
    "        * `max_length`- The maximum total source sequence length after tokenization.\n",
    "        * `padding`- whether to pad source sequence length to max_length. \n",
    "        * `mode`- which mode the sequence will use. i.e. <encoder-only>, <decoder-only>, <encoder-decoder>\n",
    "        \"\"\"\n",
    "        assert mode in [\"<encoder-only>\", \"<decoder-only>\", \"<encoder-decoder>\"]\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        \n",
    "        tokens_ids = []\n",
    "        for x in inputs:\n",
    "            tokens = tokenizer.tokenize(x)\n",
    "            if mode == \"<encoder-only>\":\n",
    "                tokens = tokens[:max_length-4]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]                \n",
    "            elif mode == \"<decoder-only>\":\n",
    "                tokens = tokens[-(max_length-3):]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens\n",
    "            else:\n",
    "                tokens = tokens[:max_length-5]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "                \n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            tokens_ids.append(tokens_id)\n",
    "\n",
    "        if padding:\n",
    "            cur_max_length = len(max(tokens_ids, key=len))\n",
    "            tokens_ids = list(map(lambda l: l + [self.config.pad_token_id] * (cur_max_length-len(l)), tokens_ids))\n",
    "        return tokens_ids\n",
    "            \n",
    "    def decode(self, source_ids):   \n",
    "        \"\"\" Convert token ids to string \"\"\"      \n",
    "        predictions = []\n",
    "        for x in source_ids:\n",
    "            prediction = []\n",
    "            for y in x:\n",
    "                t = y.cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                prediction.append(text)        \n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    def forward(self, source_ids):   \n",
    "        \"\"\" Obtain token embeddings and sentence embeddings \"\"\"\n",
    "        mask = source_ids.ne(self.config.pad_token_id)\n",
    "        token_embeddings = self.model(source_ids,attention_mask = mask.unsqueeze(1) * mask.unsqueeze(2))[0]\n",
    "        sentence_embeddings = (token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)\n",
    "        return token_embeddings, sentence_embeddings       \n",
    "\n",
    "    def generate(self, source_ids, decoder_only = True, eos_id = None, beam_size = 5, max_length = 64):\n",
    "        \"\"\" Generate sequence given context (source_ids) \"\"\"\n",
    "        \n",
    "        # Set encoder mask attention matrix: bidirectional for <encoder-decoder>, unirectional for <decoder-only>\n",
    "        if decoder_only:\n",
    "            mask = self.bias[:,:source_ids.size(-1),:source_ids.size(-1)]\n",
    "        else:\n",
    "            mask = source_ids.ne(self.config.pad_token_id)\n",
    "            mask = mask.unsqueeze(1) * mask.unsqueeze(2)  \n",
    "            \n",
    "        if eos_id is None:\n",
    "            eos_id = self.config.eos_token_id\n",
    "        \n",
    "        device = source_ids.device\n",
    "        \n",
    "        # Decoding using beam search\n",
    "        preds = []       \n",
    "        zero = torch.LongTensor(1).fill_(0).to(device)   \n",
    "        source_len = list(source_ids.ne(1).sum(-1).cpu().numpy())\n",
    "        length = source_ids.size(-1)\n",
    "        encoder_output = self.model(source_ids,attention_mask=mask)\n",
    "        for i in range(source_ids.shape[0]):\n",
    "            context = [[x[i:i+1,:,:source_len[i]].repeat(beam_size,1,1,1) for x in y] \n",
    "                     for y in encoder_output.past_key_values]\n",
    "            beam = Beam(beam_size,eos_id,device)\n",
    "            input_ids = beam.getCurrentState().clone()\n",
    "            context_ids = source_ids[i:i+1,:source_len[i]].repeat(beam_size,1)\n",
    "            out = encoder_output.last_hidden_state[i:i+1,:source_len[i]].repeat(beam_size,1,1)\n",
    "            for _ in range(max_length): \n",
    "                if beam.done():\n",
    "                    break\n",
    "                if _ == 0: \n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = beam.getCurrentState().clone()\n",
    "                else:\n",
    "                    length = context_ids.size(-1)+input_ids.size(-1)\n",
    "                    out = self.model(input_ids,attention_mask=self.bias[:,context_ids.size(-1):length,:length],\n",
    "                                       past_key_values=context).last_hidden_state\n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = torch.cat((input_ids,beam.getCurrentState().clone()),-1)\n",
    "            hyp = beam.getHyp(beam.getFinal())\n",
    "            pred = beam.buildTargetTokens(hyp)[:beam_size]\n",
    "            pred = [torch.cat([x.view(-1) for x in p]+[zero]*(max_length-len(p))).view(1,-1) for p in pred]\n",
    "            preds.append(torch.cat(pred,0).unsqueeze(0))\n",
    "\n",
    "        preds = torch.cat(preds,0)    \n",
    "\n",
    "        return preds  \n",
    "    \n",
    "\n",
    "    \n",
    "class Beam(object):\n",
    "    def __init__(self, size, eos, device):\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.FloatTensor(size).zero_().to(device)\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [torch.LongTensor(size).fill_(0).to(device)]\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eosTop = False\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        batch = self.nextYs[-1].view(-1, 1)\n",
    "        return batch\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "        Parameters:\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = bestScoresId // numWords\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                s = self.scores[i]\n",
    "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >= self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished=[]\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
    "        return self.finished[:self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps=[]\n",
    "        for _,timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j+1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "    \n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence=[]\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok==self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-07-13T18:49:57.298340Z",
     "iopub.status.busy": "2022-07-13T18:49:57.296059Z",
     "iopub.status.idle": "2022-07-13T18:50:13.646318Z",
     "shell.execute_reply": "2022-07-13T18:50:13.645538Z",
     "shell.execute_reply.started": "2022-07-13T18:49:57.298297Z"
    },
    "papermill": {
     "duration": 11.138926,
     "end_time": "2022-05-22T21:58:50.848367",
     "exception": false,
     "start_time": "2022-05-22T21:58:39.709441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reload_model(preload_from_working=False, preload_from_test=None):\n",
    "    global unixcoder_model\n",
    "    global device\n",
    "    default_model_name = \"../input/unixcoderbase\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state_dict = None\n",
    "    if preload_from_test is not None:\n",
    "        state_dict = \"../input/ai4code-train-ds/\" + preload_from_test\n",
    "        print(\"Preloading from test..\", state_dict)\n",
    "    unixcoder_model = UniXcoder(model_name=default_model_name, state_dict=state_dict)\n",
    "    unixcoder_model.to(device)\n",
    "\n",
    "reload_model()\n",
    "\n",
    "df_orders = pd.read_csv(\n",
    "    data_dir / 'train_orders.csv',\n",
    "    index_col='id',\n",
    "    squeeze=True,\n",
    ").str.split()  # Split the string representation of cell_ids into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.648799Z",
     "iopub.status.busy": "2022-07-13T18:50:13.648520Z",
     "iopub.status.idle": "2022-07-13T18:50:13.654917Z",
     "shell.execute_reply": "2022-07-13T18:50:13.653847Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.648764Z"
    },
    "papermill": {
     "duration": 0.970749,
     "end_time": "2022-05-22T21:58:54.169385",
     "exception": false,
     "start_time": "2022-05-22T21:58:53.198636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embedding(nb, cell_id):\n",
    "    start = time.time()\n",
    "    cell = nb.loc[cell_id]\n",
    "    tokens = unixcoder_model.tokenize([cell['source']],max_length=512,mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens).to(device)\n",
    "    _,embeddings = unixcoder_model(source_ids)\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.656882Z",
     "iopub.status.busy": "2022-07-13T18:50:13.656567Z",
     "iopub.status.idle": "2022-07-13T18:50:13.664816Z",
     "shell.execute_reply": "2022-07-13T18:50:13.664201Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.656849Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_tokens(text):\n",
    "    tokens = unixcoder_model.tokenize([text],max_length=512,mode=\"<encoder-only>\")\n",
    "    return torch.tensor(tokens).to(device)\n",
    "\n",
    "def get_texts_tokens(texts):\n",
    "    tokens = unixcoder_model.tokenize(texts,max_length=512,mode=\"<encoder-only>\", padding=True)\n",
    "    return torch.tensor(tokens).to(device)\n",
    "    \n",
    "\n",
    "def get_text_embedding(text):\n",
    "    source_ids = get_text_tokens(text)\n",
    "    _,embeddings = unixcoder_model(source_ids)\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.666928Z",
     "iopub.status.busy": "2022-07-13T18:50:13.666722Z",
     "iopub.status.idle": "2022-07-13T18:50:13.680989Z",
     "shell.execute_reply": "2022-07-13T18:50:13.680205Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.666906Z"
    },
    "papermill": {
     "duration": 5.202493,
     "end_time": "2022-05-22T21:58:59.401928",
     "exception": false,
     "start_time": "2022-05-22T21:58:54.199435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_preproc = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_nb_embeddings(nb):\n",
    "    start = time.time()\n",
    "\n",
    "    res = {}\n",
    "    \n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    code_cell_ids = code_cells['cell_id'].values.tolist()\n",
    "        \n",
    "\n",
    "#     TODO: maybe different size?\n",
    "    batch_size = 10\n",
    "    n_chunks = len(nb) / min(len(nb), batch_size)\n",
    "\n",
    "    nb = nb.sort_values(by=\"source\", key=lambda x: x.str.len())\n",
    "    for nb in np.array_split(nb, n_chunks):\n",
    "        # TODO: different max_length?\n",
    "        \n",
    "        texts = []\n",
    "        for key,text in zip(nb['source'].index, nb['source'].to_numpy()):\n",
    "            if key in code_cell_ids or not run_preproc:\n",
    "                texts.append(text)\n",
    "            else:\n",
    "                texts.append(text.lower())\n",
    "        \n",
    "        tokens = unixcoder_model.tokenize(texts,max_length=512,mode=\"<encoder-only>\", padding=True)\n",
    "        source_ids = torch.tensor(tokens).to(device)\n",
    "        _,embeddings = unixcoder_model(source_ids)\n",
    "        normalized = torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()\n",
    "\n",
    "        \n",
    "        for key, val in zip(nb['source'].index, normalized):\n",
    "            res[key] = val\n",
    "            \n",
    "    res['END'] = get_text_embedding('END')\n",
    "    \n",
    "    return res\n",
    "\n",
    "def get_source(cell_id):\n",
    "    return nb.loc[cell_id]['source']\n",
    "\n",
    "def sim(emb1, emb2):\n",
    "    return torch.einsum(\"i,i->\", emb1, emb2).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.682582Z",
     "iopub.status.busy": "2022-07-13T18:50:13.682295Z",
     "iopub.status.idle": "2022-07-13T18:50:13.694927Z",
     "shell.execute_reply": "2022-07-13T18:50:13.694132Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.682549Z"
    },
    "papermill": {
     "duration": 0.061111,
     "end_time": "2022-05-22T21:59:00.107367",
     "exception": false,
     "start_time": "2022-05-22T21:59:00.046256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\n",
    "# Actually O(N^2), but fast in practice for our data\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):  # O(N)\n",
    "        j = bisect(sorted_so_far, u)  # O(log N)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)  # O(N)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0  # total inversions in predicted ranks across all instances\n",
    "    total_2max = 0  # maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n",
    "\n",
    "def sum_scores(a, b):\n",
    "    total_inversions = a[1] + b[1]\n",
    "    total_2max = a[2] + b[2]\n",
    "    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.696481Z",
     "iopub.status.busy": "2022-07-13T18:50:13.696013Z",
     "iopub.status.idle": "2022-07-13T18:50:13.753470Z",
     "shell.execute_reply": "2022-07-13T18:50:13.752072Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.696436Z"
    },
    "papermill": {
     "duration": 0.093157,
     "end_time": "2022-05-22T21:59:00.249041",
     "exception": false,
     "start_time": "2022-05-22T21:59:00.155884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths_test = list((data_dir / 'test').glob('*.json'))\n",
    "notebooks_test = [\n",
    "    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
    "]\n",
    "test_df = (\n",
    "    pd.concat(notebooks_test)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:50.106683Z",
     "iopub.status.busy": "2022-07-13T18:50:50.106417Z",
     "iopub.status.idle": "2022-07-13T18:50:50.117473Z",
     "shell.execute_reply": "2022-07-13T18:50:50.116441Z",
     "shell.execute_reply.started": "2022-07-13T18:50:50.106655Z"
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_cell_order3_with_emb(nb, embeddings):\n",
    "    coef_mul = 1000.0\n",
    "    \n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    markdown_cells = nb[nb['cell_type'] != 'code'].reset_index(level='cell_id')\n",
    "        \n",
    "    code_cell_ids = code_cells['cell_id'].values.tolist()\n",
    "    code_cell_ids.append('END')\n",
    "    \n",
    "    order = code_cell_ids.copy()    \n",
    "    \n",
    "    for m_cell_id in markdown_cells['cell_id'].values:\n",
    "        markdown_emb = embeddings[m_cell_id]\n",
    "        sims = [sim(markdown_emb, embeddings[c]) for c in code_cell_ids]\n",
    "        max_sim = max(sims)\n",
    "        sims_probs = list(map(lambda x:math.exp((x-max_sim) * coef_mul), sims))\n",
    "        sum_probs = sum(sims_probs)\n",
    "        sims_probs = list(map(lambda x:x/sum_probs, sims_probs))\n",
    "        scores = [0.0] * len(sims_probs)\n",
    "        for i in range(len(sims)):\n",
    "            for j in range(len(sims)):\n",
    "                scores[j] += abs(i - j) * sims_probs[i]\n",
    "        best_pos = scores.index(min(scores))\n",
    "        \n",
    "        order.insert(order.index(code_cell_ids[best_pos]), m_cell_id)\n",
    "    \n",
    "    assert order[-1] == 'END'\n",
    "    order.pop()\n",
    "    return order\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_cell_order3(nb):\n",
    "    embeddings = get_nb_embeddings(nb)\n",
    "    return find_best_cell_order3_with_emb(nb, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.783352Z",
     "iopub.status.busy": "2022-07-13T18:50:13.783137Z",
     "iopub.status.idle": "2022-07-13T18:50:13.791312Z",
     "shell.execute_reply": "2022-07-13T18:50:13.790639Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.783327Z"
    },
    "papermill": {
     "duration": 1.80612,
     "end_time": "2022-05-22T21:59:16.906254",
     "exception": false,
     "start_time": "2022-05-22T21:59:15.100134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_results():\n",
    "    res = []\n",
    "\n",
    "    for id_, notebook in test_df.groupby(level=\"id\"):\n",
    "        order = find_best_cell_order3(notebook.reset_index(level='id'))\n",
    "        order = \" \".join(order)\n",
    "        \n",
    "        res.append({'id' : id_, 'cell_order' : order})\n",
    "        \n",
    "    \n",
    "    res = pd.DataFrame(res)\n",
    "    res.to_csv('submission.csv', index=False)\n",
    "\n",
    "    display(res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:13.793162Z",
     "iopub.status.busy": "2022-07-13T18:50:13.792595Z",
     "iopub.status.idle": "2022-07-13T18:50:20.772360Z",
     "shell.execute_reply": "2022-07-13T18:50:20.771627Z",
     "shell.execute_reply.started": "2022-07-13T18:50:13.793124Z"
    }
   },
   "outputs": [],
   "source": [
    "reload_model(preload_from_test=default_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:50:54.663076Z",
     "iopub.status.busy": "2022-07-13T18:50:54.662767Z",
     "iopub.status.idle": "2022-07-13T18:50:55.672597Z",
     "shell.execute_reply": "2022-07-13T18:50:55.671919Z",
     "shell.execute_reply.started": "2022-07-13T18:50:54.663040Z"
    }
   },
   "outputs": [],
   "source": [
    "save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
