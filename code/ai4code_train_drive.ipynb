{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWdX5wd0mgGk",
    "outputId": "34364728-70b3-4ad2-8fea-b6a8bb817754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from kaggle) (2.26.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kaggle) (2022.5.18.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from kaggle) (1.26.7)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle) (3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QS7j_VtQT4me",
    "outputId": "c3accb33-cf2c-49e5-8fcd-7041ecc93e3d"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai4-code/code\n"
     ]
    }
   ],
   "source": [
    "!cd /home\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TT4NJ2b2nCUe",
    "outputId": "69321c36-dd88-4de7-9696-3c1c99678e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading AI4Code.zip to /home/ai4-code/code\n",
      "100%|████████████████████████████████████████| 714M/714M [01:06<00:00, 14.7MB/s]\n",
      "100%|████████████████████████████████████████| 714M/714M [01:06<00:00, 11.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c AI4Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "irh2-RNDnPVP"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('AI4Code.zip', 'r')\n",
    "zip_ref.extractall('/home/input/AI4Code')\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDJEubbHqgW_",
    "outputId": "82ebe350-24fb-4e24-f55f-5e1dcc0162ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 12 10:38:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:1C:00.0 Off |                  Off |\n",
      "| 33%   26C    P8     3W / 230W |      0MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "6tlXn4X6jBj5",
    "outputId": "91b25acd-1c94-4897-feba-dd6de5d406c6",
    "papermill": {
     "duration": 0.056058,
     "end_time": "2022-05-22T21:58:28.980447",
     "exception": false,
     "start_time": "2022-05-22T21:58:28.924389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139256it [00:06, 20390.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only one: 118529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139256it [00:06, 20326.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good notebooks 118529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|██████████| 100/100 [00:00<00:00, 211.45it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "def is_interactive_mode():\n",
    "    return os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Interactive') == 'Interactive'\n",
    "\n",
    "data_dir = Path('/home/input/AI4Code')\n",
    "\n",
    "\n",
    "df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "# df_ancestors =df_ancestors.head(1000)\n",
    "# TODO: rewrite this to use the dataframe\n",
    "\n",
    "cnt_by_group = {}\n",
    "for id, row in tqdm(df_ancestors.iterrows()):\n",
    "    cnt_by_group[row['ancestor_id']] = cnt_by_group.get(row['ancestor_id'], 0) + 1\n",
    "\n",
    "\n",
    "cnt = pd.Series(cnt_by_group)\n",
    "print('only one:', cnt[cnt == 1].count())\n",
    "cnt.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                   color='#607c8e')\n",
    "cnt\n",
    "\n",
    "good_notebooks = []\n",
    "for id, row in tqdm(df_ancestors.iterrows()):\n",
    "    if row['parent_id'] != None and cnt_by_group[row['ancestor_id']] == 1:\n",
    "        good_notebooks.append(id)\n",
    "\n",
    "good_notebooks = pd.Series(good_notebooks)\n",
    "print('good notebooks', len(good_notebooks))\n",
    "\n",
    "all_train_nb = good_notebooks.sample(frac=0.9, random_state=787788)\n",
    "all_validate_nb = good_notebooks.drop(all_train_nb.index)\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "def load_train_nbs(num):\n",
    "    global df\n",
    "\n",
    "    paths_train = [data_dir / 'train' / '{}.json'.format(id) for id in all_train_nb.head(num)]\n",
    "    notebooks_train = [\n",
    "        read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "    ]\n",
    "    df = (\n",
    "        pd.concat(notebooks_train)\n",
    "        .set_index('id', append=True)\n",
    "        .swaplevel()\n",
    "        .sort_index(level='id', sort_remaining=False)\n",
    "    )\n",
    "\n",
    "    df\n",
    "    \n",
    "load_train_nbs(100)\n",
    "\n",
    "def get_example_nb_id(seed=0):\n",
    "    return df.index.get_level_values(0).unique()[seed]\n",
    "    \n",
    "nb_id = get_example_nb_id()\n",
    "\n",
    "def get_nb_by_id(nb_id):\n",
    "    return df.loc[nb_id]\n",
    "\n",
    "def get_example_cell_from_nb(nb):\n",
    "    return nb.index[0]\n",
    "\n",
    "def get_example_markdown_cell_from_nb(nb):\n",
    "    return nb[nb['cell_type'] == 'markdown'].index[0]\n",
    "\n",
    "def get_code_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'code'].index\n",
    "\n",
    "def get_markdown_cells(nb):\n",
    "    return nb[nb['cell_type'] == 'markdown'].index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Im0GS6I0q7Rw",
    "outputId": "7106c86a-dc70-47d8-8777-5af7ff0c7359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f_ZRQzR-jBj7",
    "papermill": {
     "duration": 6.489575,
     "end_time": "2022-05-22T21:58:39.682208",
     "exception": false,
     "start_time": "2022-05-22T21:58:33.192633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copied from: https://github.com/microsoft/CodeBERT/blob/master/UniXcoder/unixcoder.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. \n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "class UniXcoder(nn.Module):\n",
    "    def __init__(self, model_name, state_dict=None):\n",
    "        \"\"\"\n",
    "            Build UniXcoder.\n",
    "            Parameters:\n",
    "            * `model_name`- huggingface model card name. e.g. microsoft/unixcoder-base\n",
    "        \"\"\"        \n",
    "        super(UniXcoder, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.config = RobertaConfig.from_pretrained(model_name)\n",
    "        self.config.is_decoder = True\n",
    "        self.model = RobertaModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        if state_dict is not None:\n",
    "            self.model.load_state_dict(torch.load(state_dict))   \n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones((1024, 1024), dtype=torch.uint8)).view(1,1024, 1024))\n",
    "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embeddings.word_embeddings.weight\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "        self.tokenizer.add_tokens([\"<mask0>\"],special_tokens=True)\n",
    "        #self.tokenizer.add_tokens([\"<END>\"], special_tokens=True)\n",
    "          \n",
    "    def tokenize(self, inputs, mode=\"<encoder-only>\", max_length=512, padding=False):\n",
    "        \"\"\" \n",
    "        Convert string to token ids \n",
    "                \n",
    "        Parameters:\n",
    "        * `inputs`- list of input strings.\n",
    "        * `max_length`- The maximum total source sequence length after tokenization.\n",
    "        * `padding`- whether to pad source sequence length to max_length. \n",
    "        * `mode`- which mode the sequence will use. i.e. <encoder-only>, <decoder-only>, <encoder-decoder>\n",
    "        \"\"\"\n",
    "        assert mode in [\"<encoder-only>\", \"<decoder-only>\", \"<encoder-decoder>\"]\n",
    "        \n",
    "        tokenizer = self.tokenizer\n",
    "        \n",
    "        tokens_ids = []\n",
    "        for x in inputs:\n",
    "            tokens = tokenizer.tokenize(x)\n",
    "            if mode == \"<encoder-only>\":\n",
    "                tokens = tokens[:max_length-4]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]                \n",
    "            elif mode == \"<decoder-only>\":\n",
    "                tokens = tokens[-(max_length-3):]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens\n",
    "            else:\n",
    "                tokens = tokens[:max_length-5]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "                \n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            tokens_ids.append(tokens_id)\n",
    "\n",
    "        if padding:\n",
    "            cur_max_length = len(max(tokens_ids, key=len))\n",
    "            tokens_ids = list(map(lambda l: l + [self.config.pad_token_id] * (cur_max_length-len(l)), tokens_ids))\n",
    "        return tokens_ids\n",
    "            \n",
    "    def decode(self, source_ids):   \n",
    "        \"\"\" Convert token ids to string \"\"\"      \n",
    "        predictions = []\n",
    "        for x in source_ids:\n",
    "            prediction = []\n",
    "            for y in x:\n",
    "                t = y.cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                prediction.append(text)        \n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    def forward(self, source_ids):   \n",
    "        \"\"\" Obtain token embeddings and sentence embeddings \"\"\"\n",
    "        mask = source_ids.ne(self.config.pad_token_id)\n",
    "        token_embeddings = self.model(source_ids,attention_mask = mask.unsqueeze(1) * mask.unsqueeze(2))[0]\n",
    "        sentence_embeddings = (token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)\n",
    "        return token_embeddings, sentence_embeddings       \n",
    "\n",
    "    def generate(self, source_ids, decoder_only = True, eos_id = None, beam_size = 5, max_length = 64):\n",
    "        \"\"\" Generate sequence given context (source_ids) \"\"\"\n",
    "        \n",
    "        # Set encoder mask attention matrix: bidirectional for <encoder-decoder>, unirectional for <decoder-only>\n",
    "        if decoder_only:\n",
    "            mask = self.bias[:,:source_ids.size(-1),:source_ids.size(-1)]\n",
    "        else:\n",
    "            mask = source_ids.ne(self.config.pad_token_id)\n",
    "            mask = mask.unsqueeze(1) * mask.unsqueeze(2)  \n",
    "            \n",
    "        if eos_id is None:\n",
    "            eos_id = self.config.eos_token_id\n",
    "        \n",
    "        device = source_ids.device\n",
    "        \n",
    "        # Decoding using beam search\n",
    "        preds = []       \n",
    "        zero = torch.LongTensor(1).fill_(0).to(device)   \n",
    "        source_len = list(source_ids.ne(1).sum(-1).cpu().numpy())\n",
    "        length = source_ids.size(-1)\n",
    "        encoder_output = self.model(source_ids,attention_mask=mask)\n",
    "        for i in range(source_ids.shape[0]):\n",
    "            context = [[x[i:i+1,:,:source_len[i]].repeat(beam_size,1,1,1) for x in y] \n",
    "                     for y in encoder_output.past_key_values]\n",
    "            beam = Beam(beam_size,eos_id,device)\n",
    "            input_ids = beam.getCurrentState().clone()\n",
    "            context_ids = source_ids[i:i+1,:source_len[i]].repeat(beam_size,1)\n",
    "            out = encoder_output.last_hidden_state[i:i+1,:source_len[i]].repeat(beam_size,1,1)\n",
    "            for _ in range(max_length): \n",
    "                if beam.done():\n",
    "                    break\n",
    "                if _ == 0: \n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = beam.getCurrentState().clone()\n",
    "                else:\n",
    "                    length = context_ids.size(-1)+input_ids.size(-1)\n",
    "                    out = self.model(input_ids,attention_mask=self.bias[:,context_ids.size(-1):length,:length],\n",
    "                                       past_key_values=context).last_hidden_state\n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = torch.cat((input_ids,beam.getCurrentState().clone()),-1)\n",
    "            hyp = beam.getHyp(beam.getFinal())\n",
    "            pred = beam.buildTargetTokens(hyp)[:beam_size]\n",
    "            pred = [torch.cat([x.view(-1) for x in p]+[zero]*(max_length-len(p))).view(1,-1) for p in pred]\n",
    "            preds.append(torch.cat(pred,0).unsqueeze(0))\n",
    "\n",
    "        preds = torch.cat(preds,0)    \n",
    "\n",
    "        return preds  \n",
    "    \n",
    "\n",
    "    \n",
    "class Beam(object):\n",
    "    def __init__(self, size, eos, device):\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.FloatTensor(size).zero_().to(device)\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [torch.LongTensor(size).fill_(0).to(device)]\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eosTop = False\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        batch = self.nextYs[-1].view(-1, 1)\n",
    "        return batch\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "        Parameters:\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = bestScoresId // numWords\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                s = self.scores[i]\n",
    "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >= self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished=[]\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i)) \n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
    "        return self.finished[:self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps=[]\n",
    "        for _,timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j+1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "    \n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence=[]\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok==self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": false,
    "id": "JXC0KCtojBj9",
    "papermill": {
     "duration": 11.138926,
     "end_time": "2022-05-22T21:58:50.848367",
     "exception": false,
     "start_time": "2022-05-22T21:58:39.709441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reload_model(preload_from_input=False, preload_from_working=False):\n",
    "    global unixcoder_model\n",
    "    global device\n",
    "    default_model_name = \"/home/unixcoderbase\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    use_pretrained = True\n",
    "    state_dict = None\n",
    "    if preload_from_input:\n",
    "        print(\"Preloading from input...\")\n",
    "        state_dict = \"../input/ai4code-pre1000/model-1000.bin\" \n",
    "    if preload_from_working:\n",
    "        print(\"Preloading from working copy...\")\n",
    "        state_dict = \"../working/model-final.bin\"\n",
    "    unixcoder_model = UniXcoder(model_name=default_model_name, state_dict=state_dict)\n",
    "    unixcoder_model.to(device)\n",
    "\n",
    "reload_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKNHiwafs_u_",
    "outputId": "2c74c88d-e8a2-47b2-d03b-de92b9eb9bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', '://', 'google', '.', 'com']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4192, 1694, 4591, 132, 954]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = unixcoder_model.tokenizer\n",
    "# display(tokenizer)\n",
    "seq = tokenizer.tokenize(\"https://google.com\")\n",
    "print(seq)\n",
    "tokenizer.convert_tokens_to_ids(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LcULm314jBj-",
    "papermill": {
     "duration": 1.954704,
     "end_time": "2022-05-22T21:58:52.830883",
     "exception": false,
     "start_time": "2022-05-22T21:58:50.876179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_orders = pd.read_csv(\n",
    "    data_dir / 'train_orders.csv',\n",
    "    index_col='id',\n",
    ").squeeze(\"columns\").str.split()  # Split the string representation of cell_ids into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6BbTM2bCjBj_",
    "papermill": {
     "duration": 0.970749,
     "end_time": "2022-05-22T21:58:54.169385",
     "exception": false,
     "start_time": "2022-05-22T21:58:53.198636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embedding(nb, cell_id):\n",
    "    start = time.time()\n",
    "    cell = nb.loc[cell_id]\n",
    "    tokens = unixcoder_model.tokenize([cell['source']],max_length=512,mode=\"<encoder-only>\")\n",
    "    source_ids = torch.tensor(tokens).to(device)\n",
    "    _,embeddings = unixcoder_model(source_ids)\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGe9DqTYjBj_",
    "outputId": "453ee806-6c1b-45a5-fa38-12b7b80d53d7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    6,    2, 2553,    2]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_tokens(text):\n",
    "    tokens = unixcoder_model.tokenize([text],max_length=512,mode=\"<encoder-only>\")\n",
    "    return torch.tensor(tokens).to(device)\n",
    "\n",
    "def get_texts_tokens(texts):\n",
    "    tokens = unixcoder_model.tokenize(texts,max_length=512,mode=\"<encoder-only>\", padding=True)\n",
    "    return torch.tensor(tokens).to(device)\n",
    "    \n",
    "\n",
    "def get_text_embedding(text):\n",
    "    source_ids = get_text_tokens(text)\n",
    "    _,embeddings = unixcoder_model(source_ids)\n",
    "    return torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()[0]\n",
    "    \n",
    "get_texts_tokens(['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4igr2OZFjBj_",
    "papermill": {
     "duration": 5.202493,
     "end_time": "2022-05-22T21:58:59.401928",
     "exception": false,
     "start_time": "2022-05-22T21:58:54.199435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_nb_embeddings(nb):\n",
    "    start = time.time()\n",
    "\n",
    "    res = {}\n",
    "\n",
    "#     TODO: maybe different size?\n",
    "    batch_size = 8\n",
    "    n_chunks = len(nb) / min(len(nb), batch_size)\n",
    "\n",
    "    nb = nb.sort_values(by=\"source\", key=lambda x: x.str.len())\n",
    "    for nb in np.array_split(nb, n_chunks):\n",
    "        # TODO: different max_length?\n",
    "        tokens = unixcoder_model.tokenize(nb['source'].to_numpy(),max_length=512,mode=\"<encoder-only>\", padding=True)\n",
    "        source_ids = torch.tensor(tokens).to(device)\n",
    "        _,embeddings = unixcoder_model(source_ids)\n",
    "        normalized = torch.nn.functional.normalize(embeddings, p=2, dim=1).cpu()\n",
    "\n",
    "        \n",
    "        for key, val in zip(nb['source'].index, normalized):\n",
    "            res[key] = val\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_3FRWpi6jBj_",
    "papermill": {
     "duration": 0.060091,
     "end_time": "2022-05-22T21:58:59.509537",
     "exception": false,
     "start_time": "2022-05-22T21:58:59.449446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_source(cell_id):\n",
    "    return nb.loc[cell_id]['source']\n",
    "\n",
    "def sim(emb1, emb2):\n",
    "    return torch.einsum(\"i,i->\", emb1, emb2).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIkn7_s0jBkA",
    "outputId": "135b4113-91bc-4ab1-9fde-7478e2c25b2e",
    "papermill": {
     "duration": 0.061111,
     "end_time": "2022-05-22T21:59:00.107367",
     "exception": false,
     "start_time": "2022-05-22T21:59:00.046256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test NBs: 100%|██████████| 4/4 [00:00<00:00, 131.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from bisect import bisect\n",
    "\n",
    "\n",
    "# Actually O(N^2), but fast in practice for our data\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):  # O(N)\n",
    "        j = bisect(sorted_so_far, u)  # O(log N)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)  # O(N)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0  # total inversions in predicted ranks across all instances\n",
    "    total_2max = 0  # maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n",
    "\n",
    "def sum_scores(a, b):\n",
    "    total_inversions = a[1] + b[1]\n",
    "    total_2max = a[2] + b[2]\n",
    "    return [1 - 4 * total_inversions / total_2max, total_inversions, total_2max]\n",
    "\n",
    "paths_test = list((data_dir / 'test').glob('*.json'))\n",
    "notebooks_test = [\n",
    "    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
    "]\n",
    "test_df = (\n",
    "    pd.concat(notebooks_test)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jswTvaGMjBkA",
    "papermill": {
     "duration": 5.598456,
     "end_time": "2022-05-22T21:59:05.948129",
     "exception": false,
     "start_time": "2022-05-22T21:59:00.349673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_best_cell_order2(nb):\n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    markdown_cells = nb[nb['cell_type'] != 'code'].reset_index(level='cell_id')\n",
    "    \n",
    "    embeddings = get_nb_embeddings(nb)\n",
    "    \n",
    "    code_cell_ids = code_cells['cell_id'].values\n",
    "    \n",
    "    order = code_cell_ids.tolist()    \n",
    "    \n",
    "    for m_cell_id in markdown_cells['cell_id'].values:\n",
    "        markdown_emb = embeddings[m_cell_id]\n",
    "        best_code = None\n",
    "        best_score = -123456.0\n",
    "        \n",
    "        for c_cell_id in code_cell_ids:\n",
    "            code_emb = embeddings[c_cell_id]\n",
    "            cur_sim = sim(markdown_emb, code_emb)\n",
    "            if cur_sim > best_score:\n",
    "                best_score = cur_sim\n",
    "                best_code = c_cell_id\n",
    "        \n",
    "        index = order.index(best_code)\n",
    "        order.insert(index, m_cell_id)\n",
    "        \n",
    "    return order\n",
    "\n",
    "def score_answer(nb_id, order):\n",
    "    ground_truth = [df_orders[nb_id]]\n",
    "    predictions = [order]\n",
    "\n",
    "    return kendall_tau(ground_truth, predictions)\n",
    "\n",
    "def get_random_test_nb_ids(n, random_state=1):\n",
    "    return df.reset_index(level='cell_id').sample(n=n, random_state=random_state).index.values.tolist()\n",
    "\n",
    "def calc_nb_score(nb_id):\n",
    "    return score_answer(nb_id, find_best_cell_order2(df.loc[nb_id]))\n",
    "\n",
    "def save_results():\n",
    "    res = []\n",
    "\n",
    "    for id_, notebook in test_df.groupby(level=\"id\"):\n",
    "        order = find_best_cell_order2(notebook.reset_index(level='id'))\n",
    "        order = \" \".join(order)\n",
    "        \n",
    "        res.append({'id' : id_, 'cell_order' : order})\n",
    "        \n",
    "    \n",
    "    res = pd.DataFrame(res)\n",
    "    res.to_csv('submission.csv', index=False)\n",
    "\n",
    "    display(res.head())\n",
    "    \n",
    "def get_tokens(text):\n",
    "    return unixcoder_model.tokenize([text],max_length=512,mode=\"<encoder-only>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "e7maEsGEjBkA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Partially coied from: https://github.com/microsoft/CodeBERT/blob/567dd49a4b916835f93fb95709de714b8772fea2/UniXcoder/downstream-tasks/code-search/model.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "import torch.nn as nn\n",
    "import torch    \n",
    "class Model(nn.Module):   \n",
    "    def __init__(self, encoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "      \n",
    "    def forward(self, inputs): \n",
    "        outputs = self.encoder(inputs)[1]\n",
    "        return torch.nn.functional.normalize(outputs, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WyEN0WdzjBkB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, suffix):\n",
    "    #output_dir = Path(\"../working/\")                       \n",
    "    output_dir = Path(\".\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)   \n",
    "    model_to_save = model.encoder.model\n",
    "    output_dir = os.path.join(output_dir, 'model-{}.bin'.format(suffix)) \n",
    "    torch.save(model_to_save.state_dict(), output_dir)\n",
    "    print(\"Saved model to {}\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ednqpYHvDoS",
    "outputId": "f47a6b72-5323-4992-849f-aea79a9c6149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (59.5.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.7.0-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.3/146.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=70d6780baafaeeb1796b4bd18d231f72386146bff5f3bf9a7d38f5da38bee76c\n",
      "  Stored in directory: /home/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=2631c7c62f632b1943414f50d570d40113d9589fc54968279eba666ae7d91613\n",
      "  Stored in directory: /home/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: pathtools, shortuuid, setproctitle, sentry-sdk, promise, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 promise-2.3 sentry-sdk-1.7.0 setproctitle-1.2.3 shortuuid-1.0.9 wandb-0.12.21\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOoaqyZvjBkB",
    "outputId": "0a520626-cf9f-4eb3-c894-b2d3ade5c080",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "wb_key = open(\"/home/wandb_key\", \"r\").read()\n",
    "\n",
    "wandb.login(key=wb_key)\n",
    "\n",
    "\n",
    "def init_wandb(name, config={}):\n",
    "    is_interactive = \"-interactive\" if is_interactive_mode() else \"\"\n",
    "    config=config.copy()\n",
    "    config['name']='Train'\n",
    "    \n",
    "    wandb.init(project=\"ai4code\", name=(name+is_interactive+\"jarvis\"), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "H90m-1dhrgeA",
    "outputId": "76750d69-eadf-400d-a3c8-03d2d640861c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /home/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world here link http google'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "# import fasttext\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        #return document\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "\n",
    "preprocess_text(\"hello world !!11 here is a link http://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooEaufiPoTti",
    "outputId": "e8755bcc-b615-4341-87cb-133504de904d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=5cb5d37c025bd01ec3abbd3e453692ca6ca494668c2f67d1ceb24f08423c5bf4\n",
      "  Stored in directory: /home/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "bdcab94b506c4a3fb4b9932465cf4878",
      "f4d06bd7842a43418bccae854ee08a22",
      "74b0bc95556a49c9a4bc80dfe1371d1d",
      "778464aabf044263a9c8dc4e88437755",
      "c963789ee76c461494a518478497f32f",
      "e08a3e75f9ad4576942ff4fb28a3e024",
      "51d499a2d9f444669353fe695eed76b6",
      "de6499ee7ac94760beda7cec7e3fcc66"
     ]
    },
    "id": "s3_3pn7TjBkB",
    "outputId": "8cbddd9c-3c25-41fe-aef4-e1d550dd38ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def detect_lang(x):\n",
    "  try:\n",
    "    return detect(x)\n",
    "  except:\n",
    "    return 'other'\n",
    "\n",
    "add_end_token = True\n",
    "end_token = 'END'\n",
    "\n",
    "\n",
    "def train_on_nb(nb_id, model, optimizer, scheduler):\n",
    "    nb = get_nb_by_id(nb_id)\n",
    "    code_cell_ids = get_code_cells(nb).tolist()\n",
    "    code_cell_ids.append(end_token)\n",
    "    \n",
    "    markdown_cell_ids = get_markdown_cells(nb)\n",
    "    train_markdown_cell_ids = []\n",
    "    for cell_id in markdown_cell_ids:\n",
    "      text = nb.loc[cell_id]['source']\n",
    "      # if detect_lang(text) == 'en':\n",
    "      train_markdown_cell_ids.append(cell_id)\n",
    "\n",
    "    correct_order = df_orders.loc[nb_id]\n",
    "    if add_end_token:\n",
    "        correct_order.append(end_token)\n",
    "    \n",
    "    next_code_cell = {}\n",
    "    for i in range(len(correct_order)):\n",
    "        if correct_order[i] in markdown_cell_ids:\n",
    "            next_code_cell[correct_order[i]] = None\n",
    "            for j in range(i+1, len(correct_order)):\n",
    "                if correct_order[j] in code_cell_ids:\n",
    "                    next_code_cell[correct_order[i]] = correct_order[j]\n",
    "                    break\n",
    "            \n",
    "\n",
    "    batch_size = 8\n",
    "    num_chunks = (len(train_markdown_cell_ids) + batch_size - 1) // batch_size\n",
    "    \n",
    "    sum_loss = 0.0\n",
    "\n",
    "    if len(train_markdown_cell_ids) == 0:\n",
    "      return 0.0\n",
    "    \n",
    "    for batch_markdown_cell_ids in np.array_split(train_markdown_cell_ids, num_chunks):\n",
    "        batch_code_cells = []\n",
    "        for markdown_cell_id in batch_markdown_cell_ids:\n",
    "            need_cell = next_code_cell[markdown_cell_id]\n",
    "            if need_cell is not None and need_cell not in batch_code_cells:\n",
    "                batch_code_cells.append(need_cell)\n",
    "        \n",
    "        if len(batch_code_cells) == 0:\n",
    "            continue;\n",
    "            \n",
    "        def get_code_source(cell_id):\n",
    "            if cell_id == end_token:\n",
    "                return end_token\n",
    "            return nb.loc[cell_id]['source']\n",
    "\n",
    "        \n",
    "        def get_mark_source(cell_id):\n",
    "            if cell_id == end_token:\n",
    "                return end_token\n",
    "            return nb.loc[cell_id]['source']\n",
    "\n",
    "        code_tokens = get_texts_tokens([get_code_source(cell_id) for cell_id in batch_code_cells])\n",
    "        markdown_tokens = get_texts_tokens([get_mark_source(cell_id) for cell_id in batch_markdown_cell_ids])\n",
    "\n",
    "        code_vec = model(code_tokens)\n",
    "        markdown_vec = model(markdown_tokens)\n",
    "        scores = torch.einsum(\"ab,cb->ac\", markdown_vec, code_vec) * 1000.0\n",
    "\n",
    "        expected_order = []\n",
    "        for cell_id in batch_markdown_cell_ids:\n",
    "            if next_code_cell[cell_id] is None:\n",
    "                assert not add_end_token\n",
    "                expected_order.append(-100)\n",
    "            else:\n",
    "                expected_order.append(batch_code_cells.index(next_code_cell[cell_id]))\n",
    "\n",
    "        expected_order = torch.tensor(expected_order).to(device)\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(scores, expected_order)\n",
    "        \n",
    "        sum_loss = sum_loss + loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    scheduler.step() \n",
    "    return sum_loss / num_chunks\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "    \n",
    "    \n",
    "# for seed in tqdm(range(steps)):    \n",
    "#     run_train(seed)\n",
    "\n",
    "\n",
    "def run_train_all():\n",
    "    reload_model()\n",
    "    model = Model(unixcoder_model)\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    all = df.index.get_level_values(0).unique()\n",
    "\n",
    "    learning_rate = 3e-5\n",
    "    epochs = 1\n",
    "    steps = len(all)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = steps * epochs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_saved_time = start_time\n",
    "    save_every_s = 40 * 60\n",
    "    max_run_s = 12 * 3600\n",
    "    \n",
    "    init_wandb(name=\"10k,lr=3e-5,mark,all-lang,mul-1000-from-start\")\n",
    "    w_loss = 0.0\n",
    "    \n",
    "    for id, nb_id in enumerate(tqdm(all)):\n",
    "        cur_loss = train_on_nb(nb_id, model, optimizer, scheduler)\n",
    "        \n",
    "        w_loss = w_loss * 0.95 + cur_loss * 0.05\n",
    "        wandb.log({'loss': w_loss})\n",
    "        \n",
    "        cur_time = time.time()\n",
    "        if cur_time - last_saved_time > save_every_s:\n",
    "            last_saved_time = cur_time\n",
    "            save_model(model, id)\n",
    "        \n",
    "        if cur_time - start_time > max_run_s:\n",
    "            print('Finishing early because of timeout')\n",
    "            break\n",
    "            \n",
    "    wandb.finish()\n",
    "    save_model(model, \"final\")\n",
    "\n",
    "# run_train_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZQyQfs3QDpqu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "gbgweprwjBkB",
    "outputId": "986af571-5d91-4fc1-fe22-cccf4cb30113",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to generate model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|██████████| 10000/10000 [00:45<00:00, 219.22it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbminaiev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ai4-code/code/wandb/run-20220712_105207-1ecf76ps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bminaiev/ai4code/runs/1ecf76ps\" target=\"_blank\">10k,lr=3e-5,mark,all-lang,mul-1000-from-start-interactivejarvis</a></strong> to <a href=\"https://wandb.ai/bminaiev/ai4code\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 3868/10000 [40:01<2:28:50,  1.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ./model-3867.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7688/10000 [1:20:11<2:43:50,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ./model-7687.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:44:15<00:00,  1.60it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▄▅▄▄▆▃▅▄▅▅▃▄▃▃▃▃▃▄▄▄▂▄▂▂▂▃▄▂▃▃▁▂▃▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.72403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">10k,lr=3e-5,mark,all-lang,mul-1000-from-start-interactivejarvis</strong>: <a href=\"https://wandb.ai/bminaiev/ai4code/runs/1ecf76ps\" target=\"_blank\">https://wandb.ai/bminaiev/ai4code/runs/1ecf76ps</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220712_105207-1ecf76ps/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ./model-final.bin\n"
     ]
    }
   ],
   "source": [
    "def num_test_inputs():\n",
    "    return len(test_df.index.get_level_values(0).unique())\n",
    "\n",
    "# if num_test_inputs() != 4:\n",
    "if not is_interactive_mode() or True:\n",
    "    print('Going to generate model...')\n",
    "    # 2000 - half an hour\n",
    "    load_train_nbs(10000)\n",
    "    run_train_all()\n",
    "\n",
    "# save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "H-Y-9x8yP694"
   },
   "outputs": [],
   "source": [
    "# !mv model-final.bin model-10k-all-lang-mul1000-jarvis.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model-10k-all-lang-mul1000-jarvis.bin\n",
      "Uploaded 1xG0qPhArCUChFpkd3oZs3MoJ7GlMS-bw at 11.9 MB/s, total 503.8 MB\n"
     ]
    }
   ],
   "source": [
    "# !/home/gdrive upload -p 12wE_l-hW_ScKnP9l-cpWWRTtuC5fZD7c model-10k-all-lang-mul1000-jarvis.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7aYHOp0zvIh"
   },
   "outputs": [],
   "source": [
    "# !cp model-20312.bin drive/MyDrive/ai4-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kW_ZXuFgHSO"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31bQA5YWgRlv"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, InputExample, evaluation, losses\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hQOsZ_DaUIE"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class Part:\n",
    "  is_code: bool\n",
    "  ids: List[str]\n",
    "\n",
    "def split_parts(nb_id):\n",
    "  parts = []\n",
    "  correct_order = df_orders[nb_id]\n",
    "  nb = df.loc[nb_id]\n",
    "  i = 0\n",
    "  while i != len(correct_order):\n",
    "    j = i\n",
    "    cur_cell_type = nb.loc[correct_order[i]]['cell_type']\n",
    "    ids = []\n",
    "    while j != len(correct_order) and nb.loc[correct_order[j]]['cell_type'] == cur_cell_type:\n",
    "      ids.append(correct_order[j])\n",
    "      j = j + 1\n",
    "    parts.append(Part(cur_cell_type=='code', ids))\n",
    "    i = j\n",
    "  return parts\n",
    "\n",
    "def only_code_parts(parts):\n",
    "  return list(filter(lambda p: p.is_code, parts))\n",
    "\n",
    "def only_markdown_parts(parts):\n",
    "  return list(filter(lambda p: not p.is_code, parts))  \n",
    "\n",
    "@dataclass\n",
    "class MmDataset:\n",
    "  same_group: List[List[str]]\n",
    "  diff_group: List[List[str]]\n",
    "\n",
    "def generate_mm_dataset():\n",
    "  print('Generating markdown-markdown dataset')\n",
    "\n",
    "  all = df.index.get_level_values(0).unique()\n",
    "\n",
    "  same_group = []\n",
    "  diff_group = []\n",
    "\n",
    "  for nb_id in tqdm(all):\n",
    "    # print('nb_id:', nb_id)\n",
    "    nb = df.loc[nb_id]\n",
    "    def get_text(id):\n",
    "      return nb.loc[id]['source']\n",
    "    parts = split_parts(nb_id)\n",
    "    markdown_parts = only_markdown_parts(parts)\n",
    "    for i in range(len(markdown_parts)):\n",
    "      part = markdown_parts[i]\n",
    "      if len(part.ids) > 1 and random.getrandbits(1):\n",
    "        c1,c2 = random.sample(part.ids, 2)\n",
    "        same_group.append([get_text(c1), get_text(c2)])\n",
    "      else:\n",
    "        j = random.randint(0, len(markdown_parts) - 1)\n",
    "        if j != i:\n",
    "          c1 = random.choice(part.ids)\n",
    "          c2 = random.choice(markdown_parts[j].ids)\n",
    "          diff_group.append([get_text(c1), get_text(c2)])\n",
    "\n",
    "  return MmDataset(same_group, diff_group)\n",
    "  \n",
    "\n",
    "\n",
    "# dataloader = generate_mm_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrvmJOeiib71"
   },
   "outputs": [],
   "source": [
    "def mm_dataloader():\n",
    "  mm_dataset = generate_mm_dataset()\n",
    "  train_examples = [InputExample(texts=texts, label=1.0) for texts in mm_dataset.same_group] + [InputExample(texts=texts, label=0.0) for texts in mm_dataset.diff_group]\n",
    "  train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "  return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Exf1FRDhrNe"
   },
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('nq-distilbert-base-v1') # TODO: change model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzPijqX3lTwo"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "import seaborn as sns\n",
    "\n",
    "def test():\n",
    "  mm_dataset = generate_mm_dataset()\n",
    "\n",
    "  first  = [x[0] for x in mm_dataset.same_group] + [x[0] for x in mm_dataset.diff_group]\n",
    "  second = [x[1] for x in mm_dataset.same_group] + [x[1] for x in mm_dataset.diff_group]\n",
    "\n",
    "  embeddings1 = model.encode(first, batch_size= 8, show_progress_bar=True, convert_to_numpy=True)\n",
    "  embeddings2 = model.encode(second, batch_size= 8, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "  labels = [1]*len(mm_dataset.same_group) + [0]*len(mm_dataset.diff_group)\n",
    "\n",
    "  cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
    "\n",
    "  sz1 = len(mm_dataset.same_group)\n",
    "  print(cosine_scores[:sz1].mean())\n",
    "  print(cosine_scores[sz1:].mean())\n",
    "\n",
    "\n",
    "  # sns.distplot(cosine_scores[:sz1], label = \"1\")\n",
    "  # sns.distplot(cosine_scores[sz1:], label = \"0\")\n",
    "\n",
    "  tmp_df = pd.DataFrame({\"s1\":first,\"s2\":second, \"lbl\":labels, \"cos\": cosine_scores}).assign(delta = lambda x: np.abs(x.lbl - x.cos)).sort_values(\"delta\", ascending=True)\n",
    "\n",
    "  # getting correct incorrect match\n",
    "  scores = []\n",
    "  for x in range(10,90,1):\n",
    "      scores.append( ( x/100, tmp_df.assign(pred = lambda xx: xx.cos > x/100 )\\\n",
    "            .assign(correct = lambda xxx: xxx.pred == xxx.lbl).correct.mean() ) )\n",
    "      \n",
    "  best_score = sorted(scores, key = lambda x: x[1], reverse = True)[0]\n",
    "  print(\"Best accuracy of {} using threshold {}\".format(best_score[1], best_score[0])) \n",
    "\n",
    "  print(\"\\nBad predictions - \")\n",
    "  return tmp_df.head(50)\n",
    "            \n",
    "\n",
    "\n",
    "# test()\n",
    "# print('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6WKtP0eqBlh"
   },
   "outputs": [],
   "source": [
    "# train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# train_dataloader = mm_dataloader()\n",
    "\n",
    "# #Tune the model\n",
    "# model.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "#           epochs=1, \n",
    "#           warmup_steps= 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qcCFfSAqQeq"
   },
   "outputs": [],
   "source": [
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdF3OXaQg8H6"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IlVR1YVdCAP"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from langdetect import detect\n",
    "\n",
    "def detect_lang(x):\n",
    "  try:\n",
    "    return detect(x)\n",
    "  except:\n",
    "    return 'other'\n",
    "\n",
    "def run_train_all():\n",
    "    all = df.index.get_level_values(0).unique()\n",
    "    texts = []\n",
    "    tokenized = []\n",
    "    langs = []\n",
    "    for id, nb_id in enumerate(tqdm(all)):\n",
    "      nb = get_nb_by_id(nb_id)\n",
    "      markdown_cell_ids = get_markdown_cells(nb)\n",
    "      cell_id = random.choice(markdown_cell_ids)\n",
    "      t = nb.loc[cell_id]['source'].lower()\n",
    "      texts.append(t)\n",
    "      langs.append(detect_lang(t))\n",
    "      tokenized.append(unixcoder_model.tokenizer.tokenize(t))\n",
    "\n",
    "    return pd.DataFrame(data={'text':texts, 'lang' : langs, 'tokenized':tokenized})\n",
    "\n",
    " \n",
    "# run_train_all()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai4code-train-drive.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "51d499a2d9f444669353fe695eed76b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b0bc95556a49c9a4bc80dfe1371d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51d499a2d9f444669353fe695eed76b6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de6499ee7ac94760beda7cec7e3fcc66",
      "value": 1
     }
    },
    "778464aabf044263a9c8dc4e88437755": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdcab94b506c4a3fb4b9932465cf4878": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4d06bd7842a43418bccae854ee08a22",
       "IPY_MODEL_74b0bc95556a49c9a4bc80dfe1371d1d"
      ],
      "layout": "IPY_MODEL_778464aabf044263a9c8dc4e88437755"
     }
    },
    "c963789ee76c461494a518478497f32f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6499ee7ac94760beda7cec7e3fcc66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e08a3e75f9ad4576942ff4fb28a3e024": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4d06bd7842a43418bccae854ee08a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c963789ee76c461494a518478497f32f",
      "placeholder": "​",
      "style": "IPY_MODEL_e08a3e75f9ad4576942ff4fb28a3e024",
      "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
