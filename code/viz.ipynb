{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertviz\n",
    "%pip install jupyterlab\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from wandb_helper import init_wandb\n",
    "import wandb_helper\n",
    "import wandb\n",
    "from state import State\n",
    "\n",
    "config = config.get_default_config()\n",
    "wandb_helper.login(config)\n",
    "state = State(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.load_train_nbs_tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roberta_model_wc\n",
    "from roberta_model_wc import MyRobertaModel\n",
    "\n",
    "\n",
    "model = MyRobertaModel(state, preload_state=\"roberta-wc-model-30k-20cs-5e-5.bin\")\n",
    "# model = MyRobertaModel()\n",
    "model.to(state.device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import metric\n",
    "from metric import Score\n",
    "from common import get_code_cells, get_markdown_cells\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "from roberta_model_wc import LearnSample\n",
    "from roberta_model_wc import MyRobertaModel\n",
    "import torch\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "@dataclass\n",
    "class OneCell:\n",
    "    score: float\n",
    "    cell_id: str\n",
    "    cell_type: str\n",
    "\n",
    "\n",
    "def predict_order(state: State, nb, model: MyRobertaModel):\n",
    "    code_cells = get_code_cells(nb)\n",
    "    markdown_cells = get_markdown_cells(nb)\n",
    "\n",
    "    code_texts = []\n",
    "    for cell_id in code_cells:\n",
    "        code_texts.append(nb.loc[cell_id]['source'])\n",
    "                \n",
    "    samples = list(map(lambda x: LearnSample(text=nb.loc[x]['source'], relative_position=0.0, code_texts=code_texts), markdown_cells))\n",
    "    scores = model.predict_two_step(state, samples)\n",
    "\n",
    "    num_code_cells = len(code_cells)\n",
    "    cells = [OneCell(cell_id=cell_id, score=score*num_code_cells, cell_type='markdown') for (cell_id, score) in zip(markdown_cells, scores)] \\\n",
    "        + [OneCell(cell_id=cell_id, score=pos + 0.5, cell_type='code')\n",
    "           for (pos, cell_id) in enumerate(code_cells)]\n",
    "    cells.sort(key=lambda x: x.score)\n",
    "\n",
    "    return list(map(lambda x: x.cell_id, cells))\n",
    "\n",
    "\n",
    "def test(state: State):\n",
    "    print('Start testing')\n",
    "    df = state.cur_train_nbs\n",
    "    all = df.index.get_level_values(0).unique()\n",
    "    \n",
    "    for cnt, nb_id in enumerate(tqdm(all)):\n",
    "        nb = df.loc[nb_id]\n",
    "\n",
    "        code_cells = get_code_cells(nb)\n",
    "        markdown_cells = get_markdown_cells(nb)\n",
    "        \n",
    "        if len(code_cells) == 5:\n",
    "            print('nb_id:', nb_id)\n",
    "            print('cnt code:', len(code_cells))\n",
    "            print('cnt markdown:', len(markdown_cells))\n",
    "            display(nb)\n",
    "\n",
    "def test2(state: State, nb_id, markdown_id):\n",
    "    df = state.cur_train_nbs\n",
    "    nb = df.loc[nb_id]\n",
    "\n",
    "    code_cells = get_code_cells(nb)\n",
    "    markdown_cells = get_markdown_cells(nb)\n",
    "    \n",
    "    display(nb)\n",
    "\n",
    "    code_texts = []\n",
    "    for cell_id in code_cells:\n",
    "        code_texts.append(nb.loc[cell_id]['source'])\n",
    "                \n",
    "    sample = LearnSample(text=nb.loc[markdown_id]['source'], relative_position=0.0, code_texts=code_texts)\n",
    "    scores = model.predict(state, [sample])\n",
    "\n",
    "    encoded = model.encode_sample(sample, cnt_codes=10)\n",
    "    tokens = model.tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "    print('tokens:', tokens)\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    # display(encoded)\n",
    "\n",
    "    res = model.roberta(input_ids=torch.LongTensor([encoded['input_ids']]), output_attentions=True)\n",
    "    attention = res['attentions']\n",
    "\n",
    "    return model_view(attention, tokens, include_layers=[4])\n",
    "\n",
    "\n",
    "\n",
    "# test(state)\n",
    "test2(state, nb_id='df29fbc69f9fd7', markdown_id='789c0cd3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = \"microsoft/xtremedistil-l12-h384-uncased\"  # Find popular HuggingFace models here: https://huggingface.co/models\n",
    "input_text = \"The cat sat on the mat\"  \n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d4c7731748faf67a6b8ce01c6c5d4488a25691d99afc96e6b91ec13b7fca11a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
