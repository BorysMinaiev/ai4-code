{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Im0GS6I0q7Rw",
    "outputId": "7106c86a-dc70-47d8-8777-5af7ff0c7359"
   },
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from wandb_helper import init_wandb\n",
    "import wandb_helper\n",
    "import wandb\n",
    "from state import State\n",
    "\n",
    "config = config.get_default_config()\n",
    "wandb_helper.login(config)\n",
    "state = State(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.load_train_nbs_range(from_=100000, to_=105000)\n",
    "# state.load_train_nbs_tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_model\n",
    "from graph_model import MyGraphModel\n",
    "\n",
    "\n",
    "graph3_model = MyGraphModel(state, preload_state=\"graph3-model-epoch1.bin\")\n",
    "graph3_model.to(state.device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unixcoder\n",
    "\n",
    "unixcoder_model = unixcoder.reload_model(state, \"model-epoch1.5.bin\")\n",
    "print('Unixcoder model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import S\n",
    "from cosine_train import end_token\n",
    "import numpy as np\n",
    "from common import sim, get_probs_by_embeddings, get_best_pos_by_probs\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    text:str\n",
    "    graph3_pos:float\n",
    "    unix_pos:float\n",
    "    total_cells:int\n",
    "    md_cells:int\n",
    "    code_cells:int\n",
    "    part_code_cells:float\n",
    "    target_pos:float\n",
    "\n",
    "\n",
    "\n",
    "def gen_nb_samples(nb, graph3_embeddings, unix_embeddings, correct_order):\n",
    "    code_cells = nb[nb['cell_type'] == 'code'].reset_index(level='cell_id')\n",
    "    markdown_cells = nb[nb['cell_type'] != 'code'].reset_index(level='cell_id')\n",
    "        \n",
    "    code_cell_ids = code_cells['cell_id'].values.tolist()\n",
    "    code_cell_ids.append('END')\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    md_cells = len(markdown_cells)\n",
    "    code_cells = len(code_cells)\n",
    "    total_cells = md_cells + code_cells\n",
    "    part_code_cells = code_cells / total_cells\n",
    "    \n",
    "    for m_cell_id in markdown_cells['cell_id'].values:\n",
    "        text = nb[m_cell_id]['source']\n",
    "        graph_sims_probs = get_probs_by_embeddings(graph3_embeddings, m_cell_id, code_cell_ids, 25.0)\n",
    "        unix_sims_probs = get_probs_by_embeddings(unix_embeddings, m_cell_id, code_cell_ids, 1000.0)\n",
    "        \n",
    "        graph3_pos = get_best_pos_by_probs(graph_sims_probs)\n",
    "        unix_pos = get_best_pos_by_probs(graph_sims_probs)\n",
    "\n",
    "        target_pos = correct_order.index(m_cell_id) / len(correct_order) * len(code_cells)\n",
    "        samples.append(Sample(text=text, graph3_pos=graph3_pos, unix_pos=unix_pos, total_cells=total_cells, md_cells=md_cells, code_cells=code_cells, part_code_cells=part_code_cells, target_pos=target_pos))\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import metric\n",
    "from metric import Score\n",
    "from common import get_code_cells, get_markdown_cells, split_into_batches\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "from graph_model import Sample\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen_samples(state: State, nb, graph3_model: MyGraphModel, unixcoder_model, correct_order):\n",
    "    graph3_embeddings = graph_model.get_nb_embeddings(state, graph3_model, nb)\n",
    "    unix_embeddings = unixcoder.get_nb_embeddings(state, unixcoder_model, nb)    \n",
    "    return gen_nb_samples(nb, graph3_embeddings, unix_embeddings, correct_order)\n",
    "\n",
    "def gen_all_samples(state: State, graph3_model: MyGraphModel, unixcoder_model):\n",
    "    graph3_model.eval()\n",
    "    unixcoder_model.eval()\n",
    "    print('Start generating sample points')\n",
    "    df = state.cur_train_nbs\n",
    "    all = df.index.get_level_values(0).unique()\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for cnt, nb_id in enumerate(tqdm(all)):\n",
    "        nb = df.loc[nb_id]\n",
    "        samples += gen_nb_samples(state, nb, graph3_model, unixcoder_model, correct_order=state.df_orders.loc[nb_id])\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: do I need this?\n",
    "# state.config.batch_size = 30\n",
    "samples = gen_all_samples(state, graph3_model, unixcoder_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "def train(state: State, samples, ensemble_model, save_to_wandb=True):\n",
    "    ensemble_model.zero_grad()\n",
    "    ensemble_model.train()\n",
    "    random.seed(787788)\n",
    "    print('Start training ensemble model:', graph3_model.name)\n",
    "    if save_to_wandb:\n",
    "        init_wandb(name='train-ensemble')\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    batches = split_into_batches(samples, state.config.batch_size)\n",
    "\n",
    "    optimizer = AdamW(ensemble_model.parameters(), lr=3e-5, eps=1e-8)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(batches))\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    for samples in tqdm(batches):\n",
    "        texts = list(map(lambda s: s.text, samples))\n",
    "        additional_features = list(map(lambda s: torch.FloatTensor(\n",
    "            [s.graph3_pos, s.unix_pos, s.total_cells, s.md_cells, s.code_cells, s.part_code_cells]), samples))\n",
    "        to_mul = list(map(lambda s: torch.FloatTensor([s.graph3_pos, s.unix_pos]), samples))\n",
    "        pred = ensemble_model(texts, additional_features)\n",
    "        pred = torch.einsum(\"ab,ab->a\", pred, to_mul)\n",
    "\n",
    "        target = list(map(lambda s: s.target_pos, samples))\n",
    "        target = torch.tensor(target).to(state.device)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if save_to_wandb:\n",
    "            wandb.log({'ensemble-loss': loss.item()})\n",
    "\n",
    "    if save_to_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "ensemble_model = unixcoder.EnsembleModel(\n",
    "    unixcoder.reload_model(state, state_dict=None))\n",
    "train(state, samples, save_to_wandb=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai4code-train-drive.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d4c7731748faf67a6b8ce01c6c5d4488a25691d99afc96e6b91ec13b7fca11a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "51d499a2d9f444669353fe695eed76b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74b0bc95556a49c9a4bc80dfe1371d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51d499a2d9f444669353fe695eed76b6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de6499ee7ac94760beda7cec7e3fcc66",
      "value": 1
     }
    },
    "778464aabf044263a9c8dc4e88437755": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdcab94b506c4a3fb4b9932465cf4878": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4d06bd7842a43418bccae854ee08a22",
       "IPY_MODEL_74b0bc95556a49c9a4bc80dfe1371d1d"
      ],
      "layout": "IPY_MODEL_778464aabf044263a9c8dc4e88437755"
     }
    },
    "c963789ee76c461494a518478497f32f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6499ee7ac94760beda7cec7e3fcc66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e08a3e75f9ad4576942ff4fb28a3e024": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4d06bd7842a43418bccae854ee08a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c963789ee76c461494a518478497f32f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e08a3e75f9ad4576942ff4fb28a3e024",
      "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
